{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ATAE-LSTM[餐厅评价].ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5voBBnBzMUXS",
        "outputId": "7f50b74b-54d3-4775-8d6a-f3dbc362e7f3"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import os\n",
        "from torch.nn import init\n",
        "from torchtext.legacy import data\n",
        "from torchtext.vocab import Vectors\n",
        "from torchtext.vocab import GloVe\n",
        "import time\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClaTSbHLMZpZ",
        "outputId": "025ab598-9ffd-4dfe-d42a-2d8f52b389bc"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.42B.300d.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-25 11:43:54--  http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.42B.300d.zip [following]\n",
            "--2021-03-25 11:43:54--  https://nlp.stanford.edu/data/glove.42B.300d.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip [following]\n",
            "--2021-03-25 11:43:55--  http://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1877800501 (1.7G) [application/zip]\n",
            "Saving to: ‘glove.42B.300d.zip’\n",
            "\n",
            "glove.42B.300d.zip  100%[===================>]   1.75G  5.13MB/s    in 6m 10s  \n",
            "\n",
            "2021-03-25 11:50:06 (4.83 MB/s) - ‘glove.42B.300d.zip’ saved [1877800501/1877800501]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu28abtcMgwK",
        "outputId": "3598e200-632e-4095-fce5-c603e319fc38"
      },
      "source": [
        "!unzip glove*.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.42B.300d.zip\n",
            "  inflating: glove.42B.300d.txt      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UM7HznyMxxJ",
        "outputId": "11ea6a61-d038-4503-de75-1ee324ef4e07"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "print('Indexing word vectors.')\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open('glove.42B.300d.txt', encoding='utf-8') #need to be done...\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indexing word vectors.\n",
            "Found 1917494 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "7F04QHQVM0nW",
        "outputId": "00f3ac08-e9ea-48a9-8a52-05ac8d52f211"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv('./traindata.csv', sep='\\t', header= None)\n",
        "train = train.loc[:, [4,2,0]]\n",
        "train = train.rename(index=str, columns={ 0:\"polarity\" ,2: 'aspect',4: \"text\"})\n",
        "stn = {'positive': 1,'neutral':0,'negative': -1}\n",
        "train['polarity'] = train['polarity'].map(stn)\n",
        "#train['review'] = train['review'].apply(lambda x: nltk.word_tokenize(x))\n",
        "#dataset = dataset.rename(index=str, columns={ 0: \"sentiment\", 1: \"aspect_category\", 2: \"review\"})\n",
        "train.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>aspect</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>short and sweet – seating is great:it's romant...</td>\n",
              "      <td>seating</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This quaint and romantic trattoria is at the t...</td>\n",
              "      <td>trattoria</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The have over 100 different beers to offer thi...</td>\n",
              "      <td>food</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>THIS STAFF SHOULD BE FIRED.</td>\n",
              "      <td>STAFF</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The menu looked great, and the waiter was very...</td>\n",
              "      <td>menu</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text     aspect  polarity\n",
              "0  short and sweet – seating is great:it's romant...    seating         1\n",
              "1  This quaint and romantic trattoria is at the t...  trattoria         1\n",
              "2  The have over 100 different beers to offer thi...       food         1\n",
              "3                        THIS STAFF SHOULD BE FIRED.      STAFF        -1\n",
              "4  The menu looked great, and the waiter was very...       menu         1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "7BpEXfOENSBg",
        "outputId": "09a478b8-e9c5-4ca1-f889-07ee43aeff90"
      },
      "source": [
        "val = pd.read_csv('./devdata.csv', sep='\\t', header= None)\n",
        "val = val.loc[:, [4,2,0]]\n",
        "val = val.rename(index=str, columns={0:\"polarity\" ,2: 'aspect',4: \"text\"})\n",
        "val['polarity'] = val['polarity'].map(stn)\n",
        "#val['review']=val['review'].apply(lambda x: nltk.word_tokenize(x))\n",
        "#dataset = dataset.rename(index=str, columns={ 0: \"sentiment\", 1: \"aspect_category\", 2: \"review\"})\n",
        "val.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>aspect</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>great food, great wine list, great service in ...</td>\n",
              "      <td>neighborhood</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I thought this place was totally overrated.</td>\n",
              "      <td>place</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Fish is so very fresh.</td>\n",
              "      <td>Fish</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I showed it to the manager, and he smilingly a...</td>\n",
              "      <td>manager</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The food we ordered was excellent, although I ...</td>\n",
              "      <td>margaritas</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text        aspect  polarity\n",
              "0  great food, great wine list, great service in ...  neighborhood         1\n",
              "1        I thought this place was totally overrated.         place        -1\n",
              "2                             Fish is so very fresh.          Fish         1\n",
              "3  I showed it to the manager, and he smilingly a...       manager        -1\n",
              "4  The food we ordered was excellent, although I ...    margaritas         0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTu60XZcNUSa"
      },
      "source": [
        "train.to_csv('train.csv')\n",
        "val.to_csv('val.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzmorpupNWJa"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "label = data.Field(sequential=False)\n",
        "aspect = data.Field(sequential=False,lower=True)\n",
        "text = data.Field(sequential=True,lower=True,tokenize=word_tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoBOsNetNX_a"
      },
      "source": [
        "train, val = data.TabularDataset.splits(path='./',\n",
        "                                        skip_header=True,\n",
        "                                        train='train.csv',\n",
        "                                        validation='val.csv',\n",
        "                                        format='csv',\n",
        "                                        fields=[('',None),('text', text),('aspect', aspect),('polarity', label)\n",
        "                                                \n",
        "                                                ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIPQ03zKNdMX",
        "outputId": "ddcd8a24-5af1-4bee-848f-cab31d0bc311"
      },
      "source": [
        "vectors = Vectors(name='glove.42B.300d.txt') #need to be done...\n",
        "\n",
        "label.build_vocab(train, val)\n",
        "aspect.build_vocab(train, val, vectors=vectors)\n",
        "text.build_vocab(train, val, vectors=vectors)\n",
        "\n",
        "\n",
        "text_vocab_size = len(text.vocab)\n",
        "aspect_vocab_size = len(aspect.vocab)\n",
        "text_vector=text.vocab.vectors\n",
        "aspect_vector=aspect.vocab.vectors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1916818/1917494 [04:32<00:00, 7285.03it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn1G9UOJNk2F"
      },
      "source": [
        "batch_size=128\n",
        "train_iter, val_iter = data.Iterator.splits(\n",
        "            (train, val),\n",
        "            sort_key=lambda x: len(x.text),\n",
        "            batch_sizes=(batch_size, len(val)) # 训练集设置batch_size,验证集整个集合用于测试\n",
        "    )                            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_WdbdA8NnqH"
      },
      "source": [
        "class ATAE_LSTM(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_hiddens, num_layers):\n",
        "        super(ATAE_LSTM, self).__init__()\n",
        "        self.text_embeddings = nn.Embedding(text_vocab_size, embedding_dim)\n",
        "        self.aspect_embeddings = nn.Embedding(aspect_vocab_size, embedding_dim)\n",
        "        self.text_embeddings = nn.Embedding.from_pretrained(text_vector,\n",
        "                                                            freeze=False)\n",
        "        self.aspect_embeddings = nn.Embedding.from_pretrained(aspect_vector,\n",
        "                                                              freeze=False)\n",
        "        self.lstm = nn.LSTM(input_size=2 * embedding_dim,\n",
        "                            hidden_size=num_hiddens,\n",
        "                            num_layers=num_layers,\n",
        "                            batch_first=True,\n",
        "                            bidirectional=True)\n",
        "        self.wh = nn.Parameter(torch.Tensor(num_hiddens * 2, num_hiddens * 2))\n",
        "        self.wv = nn.Parameter(torch.Tensor(embedding_dim, embedding_dim))\n",
        "        self.omega = nn.Parameter(\n",
        "            torch.Tensor(1, embedding_dim*2))\n",
        "        self.wp = nn.Parameter(torch.Tensor(num_hiddens * 2, num_hiddens * 2))\n",
        "        self.wx = nn.Parameter(torch.Tensor(num_hiddens * 2, num_hiddens * 2))\n",
        "        self.ws = nn.Parameter(torch.Tensor(4, num_hiddens * 2))\n",
        "        nn.init.uniform_(self.wh, -0.1, 0.1)\n",
        "        nn.init.uniform_(self.wv, -0.1, 0.1)\n",
        "        nn.init.uniform_(self.omega, -0.1, 0.1)\n",
        "        nn.init.uniform_(self.wp, -0.1, 0.1)\n",
        "        nn.init.uniform_(self.wx, -0.1, 0.1)\n",
        "        nn.init.uniform_(self.ws, -0.1, 0.1)\n",
        "        self.bs = nn.Parameter(torch.zeros((4, 1)))\n",
        "\n",
        "    def forward(self, text, aspect):\n",
        "        seq_len = len(text.t())\n",
        "        e1 = self.text_embeddings(text)\n",
        "        # e1 形状是(batch_size,seq_len, embedding_dim)\n",
        "        e2 = self.aspect_embeddings(aspect).expand(e1.size())\n",
        "\n",
        "        wv = torch.cat((e1, e2), dim=2)\n",
        "        # e.g.\n",
        "        # wv torch.Size([batch_size,seq_len,2*embedding_dim])\n",
        "\n",
        "        out, (h, c) = self.lstm(wv)  # output, (h, c)\n",
        "        # out形状是(batch_size,seq_len, 2 * num_hiddens)\n",
        "        # h形状是(num_layers * num_directions, batch_size, 2*num_hiddens)\n",
        "\n",
        "        H = out.permute(0, 2, 1)\n",
        "        # H形状是(batch_size,2 * num_hiddens,seq_len)\n",
        "        #print(H.shape)\n",
        "        #print(self.wh.shape)\n",
        "\n",
        "        Wh_H = torch.matmul(self.wh, H)\n",
        "        # wh 形状是(2*num_hiddens, 2*num_hiddens)\n",
        "        # wh_H 形状是(batch_size, 2*num_hiddens, seq_len)\n",
        "        #print('Wh_H: ', Wh_H.shape)\n",
        "        Wv_Va_eN = torch.matmul(\n",
        "            self.wv,\n",
        "            self.aspect_embeddings(aspect).permute(0, 2, 1).expand(\n",
        "                -1, embedding_dim, seq_len))\n",
        "        # Wv 形状是(seq_len, seq_len) embedding_dim=2*num_hiddens\n",
        "        # Wv_Va_eN 形状是(batch_size, embedding_dim, seq_len)\n",
        "        #print('Wv_Va_eN: ', Wv_Va_eN.shape)\n",
        "\n",
        "        vh = torch.cat((Wh_H, Wv_Va_eN), dim=1)\n",
        "        # vh 形状是(batch_size, 2*embedding_dim, seq_len)\n",
        "        #print('vh: ', vh.shape)\n",
        "\n",
        "        M = torch.tanh(vh)\n",
        "        # M 形状是(batch_size, 2*embedding_dim, seq_len)\n",
        "        #print('M: ', M.shape)\n",
        "\n",
        "        alpha = F.softmax(torch.matmul(self.omega, M),dim=2)\n",
        "        # omega 形状为(1, 2*embedding_dim))\n",
        "        # alpha 形状为(batch_size, 1, seq_len)\n",
        "        #print('alpha: ', alpha.shape)\n",
        "\n",
        "        r = torch.matmul(H, alpha.permute(0, 2, 1))\n",
        "        # H形状是(batch_size,2 * num_hiddens,seq_len)\n",
        "        # r 形状为(batch_size,2*num_hiddens,1)\n",
        "        #print('r: ', r.shape)\n",
        "\n",
        "        h_star = torch.tanh(\n",
        "            torch.matmul(self.wp, r) +\n",
        "            torch.matmul(self.wx, torch.unsqueeze(H[:, :, -1], 2)))\n",
        "        # h_star形状是(batch_size,2 * num_hiddens,1)\n",
        "        #print('h_star: ', h_star.shape)\n",
        "\n",
        "        y = torch.matmul(self.ws, h_star) + self.bs  #不需要手动求softmax\n",
        "        # y 形状(batch_size,4,1)\n",
        "        \n",
        "        y = y.reshape([-1, 4])\n",
        "        # y 形状(batch_size,4)\n",
        "        # ws 形状(4, 2*num_hiddens)\n",
        "        #print('y: ', y.shape)\n",
        "\n",
        "        return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--YxCycYNq01",
        "outputId": "ad800481-600c-4d16-9a81-d335ffb852aa"
      },
      "source": [
        "embedding_dim, num_hiddens, num_layers = 300, 150, 3\n",
        "net = ATAE_LSTM(embedding_dim, num_hiddens, num_layers)\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ATAE_LSTM(\n",
            "  (text_embeddings): Embedding(2760, 300)\n",
            "  (aspect_embeddings): Embedding(676, 300)\n",
            "  (lstm): LSTM(600, 150, num_layers=3, batch_first=True, bidirectional=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zACvutwNtWK"
      },
      "source": [
        "def evaluate_accuracy(data_iter, net):\n",
        "    acc_sum, n = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(data_iter):\n",
        "            X1, X2, y = batch.text, batch.aspect, batch.polarity\n",
        "            X1 = X1.permute(1, 0)\n",
        "            X2 = X2.unsqueeze(1)\n",
        "            if isinstance(net, torch.nn.Module):\n",
        "                net.eval()  # 评估模式, 这会关闭dropout\n",
        "                acc_sum += (net(X1,\n",
        "                                X2).argmax(dim=1) == y).float().sum().item()\n",
        "                net.train()  # 改回训练模式\n",
        "            else:\n",
        "                if ('is_training'\n",
        "                        in net.__code__.co_varnames):  # 如果有is_training这个参数\n",
        "                    # 将is_training设置成False\n",
        "                    acc_sum += (net(X1, X2, is_training=False).argmax(\n",
        "                        dim=1) == y).float().sum().item()\n",
        "                else:\n",
        "                    acc_sum += (net(\n",
        "                        X1, X2).argmax(dim=1) == y).float().sum().item()\n",
        "            n += y.shape[0]\n",
        "    return acc_sum / n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQFzttmJNwW3"
      },
      "source": [
        "def train(train_iter, test_iter, net, loss, optimizer, num_epochs):\n",
        "    batch_count = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
        "        for batch_idx, batch in enumerate(train_iter):\n",
        "            X1, X2, y = batch.text, batch.aspect, batch.polarity\n",
        "            X1 = X1.permute(1, 0)\n",
        "            X2 = X2.unsqueeze(1)\n",
        "            y_hat = net(X1,X2)\n",
        "            l = loss(y_hat, y)\n",
        "            optimizer.zero_grad()\n",
        "            l.backward()\n",
        "            optimizer.step()\n",
        "            train_l_sum += l.item()\n",
        "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
        "            n += y.shape[0]\n",
        "            batch_count += 1\n",
        "        test_acc = evaluate_accuracy(test_iter, net)\n",
        "        print(\n",
        "            'epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
        "            % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n,\n",
        "               test_acc, time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOqF299JNyq9",
        "outputId": "9ba72749-1dc3-4df5-b0f4-5fcd37bb6bfb"
      },
      "source": [
        "lr, num_epochs = 0.01, 20\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "train(train_iter, val_iter, net, loss, optimizer, num_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1, loss 1.3231, train acc 0.605, test acc 0.702, time 35.8 sec\n",
            "epoch 2, loss 0.4068, train acc 0.653, test acc 0.702, time 36.4 sec\n",
            "epoch 3, loss 0.2526, train acc 0.655, test acc 0.702, time 40.1 sec\n",
            "epoch 4, loss 0.1763, train acc 0.687, test acc 0.702, time 36.4 sec\n",
            "epoch 5, loss 0.1276, train acc 0.706, test acc 0.705, time 33.5 sec\n",
            "epoch 6, loss 0.0975, train acc 0.777, test acc 0.713, time 33.0 sec\n",
            "epoch 7, loss 0.0614, train acc 0.857, test acc 0.721, time 33.8 sec\n",
            "epoch 8, loss 0.0430, train acc 0.894, test acc 0.750, time 34.6 sec\n",
            "epoch 9, loss 0.0321, train acc 0.917, test acc 0.747, time 32.7 sec\n",
            "epoch 10, loss 0.0253, train acc 0.925, test acc 0.766, time 31.8 sec\n",
            "epoch 11, loss 0.0214, train acc 0.923, test acc 0.793, time 33.3 sec\n",
            "epoch 12, loss 0.0172, train acc 0.931, test acc 0.723, time 33.8 sec\n",
            "epoch 13, loss 0.0158, train acc 0.937, test acc 0.718, time 32.3 sec\n",
            "epoch 14, loss 0.0124, train acc 0.942, test acc 0.715, time 34.9 sec\n",
            "epoch 15, loss 0.0106, train acc 0.951, test acc 0.721, time 34.6 sec\n",
            "epoch 16, loss 0.0090, train acc 0.949, test acc 0.707, time 36.2 sec\n",
            "epoch 17, loss 0.0078, train acc 0.952, test acc 0.742, time 37.5 sec\n",
            "epoch 18, loss 0.0067, train acc 0.959, test acc 0.660, time 35.1 sec\n",
            "epoch 19, loss 0.0063, train acc 0.957, test acc 0.699, time 33.8 sec\n",
            "epoch 20, loss 0.0050, train acc 0.966, test acc 0.734, time 38.2 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXgt9YO6R0Uf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}