{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "TP4_transfer_second_done.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2202d0a14c1d43cc8bbcd3269bdb5845": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_29adc4b2f92448d09f4502c18b1f8687",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dc6b9d589b644fd081bd04eda0adce48",
              "IPY_MODEL_748fbf17fced427b8c0a1f28a0990a4a"
            ]
          }
        },
        "29adc4b2f92448d09f4502c18b1f8687": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dc6b9d589b644fd081bd04eda0adce48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6e1d48ce2c094ec3a57d8b08f7b94968",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 46827520,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46827520,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bfec5c1c49ba4e4c82fbd39af81e667c"
          }
        },
        "748fbf17fced427b8c0a1f28a0990a4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_26fdfa3de1e14759aa477c2dfb67b7f3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 186MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8c2bdc94eb794416be9b83190f62fe00"
          }
        },
        "6e1d48ce2c094ec3a57d8b08f7b94968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bfec5c1c49ba4e4c82fbd39af81e667c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "26fdfa3de1e14759aa477c2dfb67b7f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8c2bdc94eb794416be9b83190f62fe00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrWnOOOd6mdR"
      },
      "source": [
        "# Small data and deep learning\n",
        "This Pratical session proposes to study several techniques for improving challenging context, in which few data and resources are available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJO2_mTX6mdW"
      },
      "source": [
        "# Introduction\n",
        "Assume we are in a context where few \"gold\" labeled data are available for training, say $\\mathcal{X}_{\\text{train}}\\triangleq\\{(x_n,y_n)\\}_{n\\leq N_{\\text{train}}}$, where $N_{\\text{train}}$ is small. A large test set $\\mathcal{X}_{\\text{test}}$ is available. A large amount of unlabeled data, $\\mathcal{X}$, is available. We also assume that we have a limited computational budget (e.g., no GPUs).\n",
        "\n",
        "For each question, write a commented *Code* or a complete answer as a *Markdown*. When the objective of a question is to report a CNN accuracy, please use the following format to report it, at the end of the question:\n",
        "\n",
        "| Model | Number of  epochs  | Train accuracy | Test accuracy |\n",
        "|------|------|------|------|\n",
        "|   XXX  | XXX | XXX | XXX |\n",
        "\n",
        "If applicable, please add the field corresponding to the  __Accuracy on Full Data__ as well as a link to the __Reference paper__ you used to report those numbers. (You do not need to train a CNN on the full CIFAR10 dataset)\n",
        "\n",
        "In your final report, please keep the logs of each training procedure you used. We will only run this jupyter if we have some doubts on your implementation. \n",
        "\n",
        "__The total file sizes should be reasonable (feasible with 2MB only!). You will be asked to hand in the notebook, together with any necessary files required to run it if any.__\n",
        "\n",
        "\n",
        "You can use https://colab.research.google.com/ to run your experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogcrRxPyBOyi",
        "outputId": "a9507b39-447c-47c1-844e-7fd6b1dedfeb"
      },
      "source": [
        "import os\r\n",
        "import glob\r\n",
        "from google.colab import drive\r\n",
        "\r\n",
        "## load train, test and validation label arrays\r\n",
        "drive = drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Jtej1A_DVYU"
      },
      "source": [
        "import torch\r\n",
        "import torchvision\r\n",
        "import torchvision.transforms as transforms\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AQ-HvnT6mdX"
      },
      "source": [
        "## Training set creation\n",
        "__Question 1 (2 points):__ Propose a dataloader or modify the file located at https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py in order to obtain a training loader that will only use the first 100 samples of the CIFAR-10 training set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxYOvTHo6mdX",
        "outputId": "602b4d9d-4c19-45ed-85a3-0f3cbc5250da"
      },
      "source": [
        "transform_train = transforms.Compose([\r\n",
        "    transforms.ToTensor(),transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\r\n",
        "\r\n",
        "transform_test = transforms.Compose([\r\n",
        "    transforms.ToTensor(),transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\r\n",
        "])\r\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\r\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\r\n"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8CYYDaLQYth"
      },
      "source": [
        "#https://www.cnblogs.com/marsggbo/p/10496696.html \r\n",
        "from torch.utils.data.sampler import SubsetRandomSampler,SequentialSampler,BatchSampler\r\n",
        "indices=list(range(len(trainset)))\r\n",
        "first100 = torch.utils.data.Subset(trainset,list(range(0,100)))\r\n",
        "train_batch_dataloader = torch.utils.data.DataLoader(first100,batch_size=5,drop_last=False,shuffle=True)#\r\n",
        "X_dataloader = torch.utils.data.DataLoader(trainset,sampler=SequentialSampler(indices[100:]),batch_size=20,drop_last=False,shuffle=False)\r\n",
        "val_batch_dataloader = torch.utils.data.DataLoader(testset,batch_size=5,drop_last=False,shuffle=False)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD0Oa9o8chIn"
      },
      "source": [
        "# https://zhuanlan.zhihu.com/p/30934236\r\n",
        "#主要讲的是dataloader和dataset的区别，dataset可以用get item/lenth GET tensor的具体数据/datset的整体长度\r\n",
        "#dataloader 主要用于对数据集进行各种取样，batch，返回的是个iter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjqkvsBY6mdX"
      },
      "source": [
        "This is our dataset $\\mathcal{X}_{\\text{train}}$, it will be used until the end of this project. The remaining samples correspond to $\\mathcal{X}$. The testing set $\\mathcal{X}_{\\text{test}}$ corresponds to the whole testing set of CIFAR-10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-1LrnqY6mdY"
      },
      "source": [
        "## Testing procedure\n",
        "__Question 2 (1.5 points):__ Explain why the evaluation of the training procedure is difficult. Propose several solutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1FRJF1_Vkwa"
      },
      "source": [
        "**#### The difficulty to evaluate the training procedures can be caused by many aspects.**\r\n",
        "\r\n",
        "1. The difficulty from the explainability\r\n",
        "\r\n",
        "    (1) Difficulty: We can directly see the accuracy score of the model. However, we can't directly see the black box in the hidden layers, for example what features do they learn and do they learn the correct features？ In some cases, even though the accuracy score is satisfying, the training procedure can be a totally mess.\r\n",
        "    \r\n",
        "    (2) Solutions: To solve this problem, we can use CAM and Grad-CAM to diaplay the heatmap to locate the key areas on which the network makes the decision to see if they catch the correct features. We can also visualize the filters in each layer and see what features has learnt by them.\r\n",
        "\r\n",
        "\r\n",
        "2. The difficulty from the dataset\r\n",
        "\r\n",
        "    (1) Difficulty: If there are noises from the data, it will consufe our evaluation of the training. ① The small amount of data will cause the over fitting and under fitting. ② The different distribution of the trainset and testset will confuse the evaluation. ③ The similarity of the trainset and testset can also influence our measurement of the training procedure. Especially in this assignment, we have an extremly small amount of data and they are not sampled randomly.\r\n",
        "    (2) Soltion: To solve these problems, we can get more data, split them randomly and in same distribution to make sure the data in good quality. Besides, we can introduce transfer learning such as data augmentation, pre-trained model, weak supervise, incorporating a priori to reduce the influence from the dataset.\r\n",
        "\r\n",
        "\r\n",
        "3. The difficulty from the property of models\r\n",
        "    (1) Difficulty: The randomness and unstability of the neural network. The deeplearing netwrok will set random initial weight and randomly shuffle the data in each training period. So we will get different results everytime. \r\n",
        "    (2) Soltion: To solve this problem, we can set a seed to gain same inital weight.\r\n",
        "    \r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIVvKa5-6mdY"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vio4vzAi6mdY"
      },
      "source": [
        "# Raw approach: the baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRTApG3w6mdZ"
      },
      "source": [
        "In this section, the goal is to train a CNN on $\\mathcal{X}_{\\text{train}}$ and compare its performance with reported numbers from the litterature. You will have to re-use and/or design a standard classification pipeline. You should optimize your pipeline to obtain the best performances (image size, data augmentation by flip, ...).\n",
        "\n",
        "The key ingredients for training a CNN are the batch size, as well as the learning rate schedule, i.e. how to decrease the learning rate as a function of the number of epochs. A possible schedule is to start the learning rate at 0.1 and decreasing it every 30 epochs by 10. In case of divergence, reduce the laerning rate. A potential batch size could be 10, yet this can be cross-validated.\n",
        "\n",
        "You can get some baselines accuracies in this paper: http://openaccess.thecvf.com/content_cvpr_2018/papers/Keshari_Learning_Structure_and_CVPR_2018_paper.pdf. Obviously, it is a different context for those researchers who had access to GPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewgPAN1z6mdZ"
      },
      "source": [
        "## ResNet architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWauNOQC6mdZ"
      },
      "source": [
        "__Question 3 (4 points):__ Write a classification pipeline for $\\mathcal{X}_{\\text{train}}$, train from scratch and evaluate a *ResNet-18* architecture specific to CIFAR10 (details about the ImageNet model can be found here: https://arxiv.org/abs/1512.03385). Please report the accuracy obtained on the whole dataset as well as the reference paper/GitHub link.\n",
        "\n",
        "*Hint:* You can re-use the following code: https://github.com/kuangliu/pytorch-cifar. During a training of 10 epochs, a batch size of 10 and a learning rate of 0.01, one obtains 40% accuracy on $\\mathcal{X}_{\\text{train}}$ (\\~2 minutes) and 20% accuracy on $\\mathcal{X}_{\\text{test}}$ (\\~5 minutes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fyam8xw96mdZ"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "class BasicBlock(nn.Module):\r\n",
        "  expansion = 1\r\n",
        "\r\n",
        "  def __init__(self, in_channel, out_channel, stride=1):\r\n",
        "    super(BasicBlock, self).__init__()\r\n",
        "    self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=3, stride=stride, padding=1, bias=False)#stride=stride make 像素进一步减半\r\n",
        "    self.bn1 = nn.BatchNorm2d(out_channel)\r\n",
        "    self.relu = nn.ReLU()\r\n",
        "    self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel, kernel_size=3, stride=1, padding=1, bias=False)#但此处的是保持像素的\r\n",
        "    self.bn2 = nn.BatchNorm2d(out_channel)\r\n",
        "    self.shortcut = nn.Sequential()\r\n",
        "    if stride != 1 or in_channel != self.expansion*out_channel:\r\n",
        "        self.shortcut = nn.Sequential(nn.Conv2d(in_channel, self.expansion*out_channel ,kernel_size=1, stride=stride, bias=False),\r\n",
        "                nn.BatchNorm2d(self.expansion*out_channel)\r\n",
        "            ) # in case intermediate layer does not match the channel size\r\n",
        "\r\n",
        "  def forward(self, x): #forward of one basic block \r\n",
        "    \r\n",
        "    out = self.relu(self.bn1(self.conv1(x)))\r\n",
        "\r\n",
        "    out = self.bn2(self.conv2(out))\r\n",
        "\r\n",
        "    out += self.shortcut(x)\r\n",
        "    out = self.relu(out) #as relu is after the PLUS action.\r\n",
        "\r\n",
        "    return out\r\n",
        "\r\n",
        "class ResNet(nn.Module):\r\n",
        "  def __init__(self, block, blocks_num, num_classes=10): \r\n",
        "    # block is in [basicblock ,bottleneck];\r\n",
        "    # blocks_num is [2,2,2,2] for 18-layer; [3,4,6,3]for 34-layer;\r\n",
        "    super(ResNet, self).__init__()\r\n",
        "    self.in_channel = 64\r\n",
        "\r\n",
        "    self.conv1 = nn.Conv2d(3, self.in_channel, 3, stride=1, padding=1, bias=False)\r\n",
        "    self.bn1 = nn.BatchNorm2d(self.in_channel)#here we can write in_channel or 64 since it is used only once,the other batch norms are defined within the block.\r\n",
        "    self.relu = nn.ReLU(inplace=True)\r\n",
        "    #self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n",
        "    self.layer1 = self._make_layer(block, 64, blocks_num[0]) #数字是：#input channels\r\n",
        "    self.layer2 = self._make_layer(block, 128, blocks_num[1], stride=2)#stride=2 since we need to make h&w=output\r\n",
        "    self.layer3 = self._make_layer(block, 256, blocks_num[2], stride=2)\r\n",
        "    self.layer4 = self._make_layer(block, 512, blocks_num[3], stride=2)\r\n",
        "   \r\n",
        "    self.fc = nn.Linear(512*block.expansion, num_classes) #output\r\n",
        " #   for m in self.modules():\r\n",
        "  #    if isinstance(m, nn.Conv2d):\r\n",
        "   #     nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')'''\r\n",
        "  \r\n",
        "  def _make_layer(self, block, channel, block_num, stride=1):\r\n",
        "    \r\n",
        "    layers = []\r\n",
        "    layers.append(block(self.in_channel, channel,stride=stride)) #here we use what we defined for block\r\n",
        "    #if downsample=none, then it was just a block without conv for x.\r\n",
        "    self.in_channel = channel*block.expansion\r\n",
        "\r\n",
        "    for _ in range(1, block_num): #block_num:[3,4,6，3] for resnet-34\r\n",
        "      layers.append(block(self.in_channel, channel)) #since only the first layer in the block needing changement of stride,the others all have stride=1\r\n",
        "    return nn.Sequential(*layers)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    x=self.relu(self.bn1(self.conv1(x)))\r\n",
        "\r\n",
        "    x=self.layer1(x)\r\n",
        "    x=self.layer2(x)\r\n",
        "    x=self.layer3(x)\r\n",
        "    x=self.layer4(x)\r\n",
        "\r\n",
        "   \r\n",
        "    x = F.avg_pool2d(x, 4)\r\n",
        "    x = torch.flatten(x,1)\r\n",
        "    x = self.fc(x)\r\n",
        "    return x\r\n",
        "\r\n",
        "def resnet18(num_classes=10, include_top=True):\r\n",
        "  return ResNet(BasicBlock, [2,2,2,2], num_classes=num_classes)\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4_6wXIt_qw6"
      },
      "source": [
        "!pip install torchnet\r\n",
        "import torchnet as tnt\r\n",
        "\r\n",
        "# define confusion matrix using tnt package\r\n",
        "confusion_matrix = tnt.meter.ConfusionMeter(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKcQIC5RA6vA"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "# create a function that creates train-val loss graph and saves the figure in a destination folder\r\n",
        "def save_graph(train_loss, val_loss, nb_epochs):\r\n",
        "    plt.plot(list(range(nb_epochs+1))[1:], train_loss)\r\n",
        "    plt.plot(list(range(nb_epochs+1))[1:], val_loss)\r\n",
        "    plt.legend(['train', 'val'])\r\n",
        "    plt.xlabel('Epochs')\r\n",
        "    plt.ylabel('Loss')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Do2N0GO7di_"
      },
      "source": [
        "from torch.optim import lr_scheduler\r\n",
        "model = resnet18().to(device)\r\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=5e-4)#, momentum=0.9, weight_decay=5e-4\r\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "epochs = 30"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qxPRlKUi7iId",
        "outputId": "ffdb5c74-b8c0-46ea-d112-4fba3cc96a38"
      },
      "source": [
        "import numpy as np\r\n",
        "save_folder = 'drive/My Drive/ADL'\r\n",
        "total_train_losses = []\r\n",
        "total_val_losses = []\r\n",
        "\r\n",
        "for epoch in range(epochs): # loop over the dataset multiple times\r\n",
        "    model.train()\r\n",
        "    train_losses = []\r\n",
        "    confusion_matrix.reset()\r\n",
        "    for i, data in enumerate(train_batch_dataloader): \r\n",
        "        # get the inputs\r\n",
        "        inputs, labels = data\r\n",
        "        inputs = inputs.to(device)\r\n",
        "        labels = labels.to(device)\r\n",
        "        # zero the parameter gradients\r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        # forward + backward + optimize\r\n",
        "        output = model(inputs)\r\n",
        "        loss = criterion(output, labels)\r\n",
        "        loss.backward()        \r\n",
        "        optimizer.step()\r\n",
        "        scheduler.step()\r\n",
        "        train_losses.append(loss.item())\r\n",
        "        confusion_matrix.add(output.data.squeeze(), labels.long())\r\n",
        "        # print statistics\r\n",
        "        if i % 5 == 0:\r\n",
        "            print('Train (epoch {}/{}) [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n",
        "                epoch, epochs, i, len(train_batch_dataloader),100.*i/len(train_batch_dataloader), loss.item()))\r\n",
        "\r\n",
        "    train_acc=(np.trace(confusion_matrix.conf)/float(np.ndarray.sum(confusion_matrix.conf))) *100\r\n",
        "    train_loss_mean = np.mean(train_losses)\r\n",
        "    total_train_losses.append(train_loss_mean)\r\n",
        "    confusion_matrix.reset()\r\n",
        "\r\n",
        "     ##VALIDATION##\r\n",
        "    with torch.no_grad():\r\n",
        "      model.eval()\r\n",
        "      val_losses = []\r\n",
        "\r\n",
        "      for i, data in enumerate(val_batch_dataloader):\r\n",
        "          inputs, labels = data\r\n",
        "          inputs = inputs.to(device)\r\n",
        "          labels = labels.to(device)\r\n",
        "          outputs=model(inputs)\r\n",
        "          loss=criterion(outputs, labels)\r\n",
        "          val_losses.append(loss.item())\r\n",
        "\r\n",
        "          confusion_matrix.add(outputs.data.squeeze(), labels)\r\n",
        "          val_losses.append(loss.item())\r\n",
        "\r\n",
        "    print('Confusion Matrix:')\r\n",
        "    print(confusion_matrix.conf)\r\n",
        "\r\n",
        "    val_acc=(np.trace(confusion_matrix.conf)/float(np.ndarray.sum(confusion_matrix.conf))) *100\r\n",
        "    val_loss_mean = np.mean(val_losses)\r\n",
        "    total_val_losses.append(val_loss_mean)\r\n",
        "\r\n",
        "    print('TRAIN_LOSS: ', '%.3f' % train_loss_mean, 'TRAIN_ACC: ', '%.3f' % train_acc)\r\n",
        "    print('VAL_LOSS: ', '%.3f' % val_loss_mean, 'VAL_ACC: ', '%.3f' % val_acc)\r\n",
        "    confusion_matrix.reset()\r\n",
        "    #write_results(ff, save_folder, epoch, train_acc, val_acc, train_loss_mean, val_loss_mean)\r\n",
        "\r\n",
        "    torch.save(model.state_dict(), save_folder + '/model_{}.pt'.format(epoch))\r\n",
        "\r\n",
        "save_graph(total_train_losses, total_val_losses, epochs)\r\n",
        "\r\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train (epoch 0/30) [0/20 (0%)]\tLoss: 2.385427\n",
            "Train (epoch 0/30) [5/20 (25%)]\tLoss: 2.755600\n",
            "Train (epoch 0/30) [10/20 (50%)]\tLoss: 2.658655\n",
            "Train (epoch 0/30) [15/20 (75%)]\tLoss: 2.375797\n",
            "Confusion Matrix:\n",
            "[[   0   81    0    0  919    0    0    0    0    0]\n",
            " [   0   14    0    0  986    0    0    0    0    0]\n",
            " [   0   18    0    0  982    0    0    0    0    0]\n",
            " [   0    4    0    0  996    0    0    0    0    0]\n",
            " [   0    2    0    0  998    0    0    0    0    0]\n",
            " [   0    6    0    0  994    0    0    0    0    0]\n",
            " [   0    0    0    0 1000    0    0    0    0    0]\n",
            " [   0    6    0    0  994    0    0    0    0    0]\n",
            " [   0   18    0    0  982    0    0    0    0    0]\n",
            " [   0    7    0    0  993    0    0    0    0    0]]\n",
            "TRAIN_LOSS:  2.470 TRAIN_ACC:  13.000\n",
            "VAL_LOSS:  2.505 VAL_ACC:  10.120\n",
            "Train (epoch 1/30) [0/20 (0%)]\tLoss: 2.240103\n",
            "Train (epoch 1/30) [5/20 (25%)]\tLoss: 2.467144\n",
            "Train (epoch 1/30) [10/20 (50%)]\tLoss: 1.637054\n",
            "Train (epoch 1/30) [15/20 (75%)]\tLoss: 1.983783\n",
            "Confusion Matrix:\n",
            "[[ 14  38 464  27 121   0   0   3   0 333]\n",
            " [  0  42 637  53  69   0   0  11   0 188]\n",
            " [  5   9 641  28 236   0   0   6   0  75]\n",
            " [  1  13 691  45 193   0   0  11   0  46]\n",
            " [  0   2 682  26 255   0   0   9   0  26]\n",
            " [  1  14 676  51 216   0   0   6   0  36]\n",
            " [  0   0 717  19 236   0   0   5   0  23]\n",
            " [  0  11 741  39 119   0   0  10   0  80]\n",
            " [  2  21 568  44 131   0   0   8   0 226]\n",
            " [  0  20 635  50  63   0   0   2   0 230]]\n",
            "TRAIN_LOSS:  1.937 TRAIN_ACC:  29.000\n",
            "VAL_LOSS:  2.427 VAL_ACC:  12.370\n",
            "Train (epoch 2/30) [0/20 (0%)]\tLoss: 1.529842\n",
            "Train (epoch 2/30) [5/20 (25%)]\tLoss: 1.896477\n",
            "Train (epoch 2/30) [10/20 (50%)]\tLoss: 1.955046\n",
            "Train (epoch 2/30) [15/20 (75%)]\tLoss: 1.603622\n",
            "Confusion Matrix:\n",
            "[[ 28 355 210   4  79   0   0   1   0 323]\n",
            " [  2 451 264  11  29   0   0  19   0 224]\n",
            " [  6 150 487  10 222   0   1  10   0 114]\n",
            " [  3 193 508  23 147   0   0  18   0 108]\n",
            " [  0  58 581  12 268   0   0  22   0  59]\n",
            " [  2 273 467  24 153   0   0  13   0  68]\n",
            " [  2  60 609   6 244   0   0  17   0  62]\n",
            " [  1 219 446   8  96   0   0  14   0 216]\n",
            " [  4 304 249   6  68   0   0   4   0 365]\n",
            " [  3 322 234   8  18   0   0   5   0 410]]\n",
            "TRAIN_LOSS:  1.737 TRAIN_ACC:  43.000\n",
            "VAL_LOSS:  2.297 VAL_ACC:  16.810\n",
            "Train (epoch 3/30) [0/20 (0%)]\tLoss: 1.917662\n",
            "Train (epoch 3/30) [5/20 (25%)]\tLoss: 1.836394\n",
            "Train (epoch 3/30) [10/20 (50%)]\tLoss: 1.872788\n",
            "Train (epoch 3/30) [15/20 (75%)]\tLoss: 1.830287\n",
            "Confusion Matrix:\n",
            "[[ 29 442 192   4  62   0   0   1   0 270]\n",
            " [  2 574 213   9  17   0   0  14   0 171]\n",
            " [  5 192 478   9 199   0   1  11   0 105]\n",
            " [  4 241 497  23 124   0   0  16   0  95]\n",
            " [  0  81 586  14 237   0   0  23   0  59]\n",
            " [  0 329 428  25 141   0   0  11   0  66]\n",
            " [  2  92 601  10 224   0   0  14   0  57]\n",
            " [  2 285 402  11  81   0   0  12   0 207]\n",
            " [  4 373 220   7  61   0   0   4   0 331]\n",
            " [  1 428 181   6  15   0   0   3   0 366]]\n",
            "TRAIN_LOSS:  1.680 TRAIN_ACC:  45.000\n",
            "VAL_LOSS:  2.304 VAL_ACC:  17.190\n",
            "Train (epoch 4/30) [0/20 (0%)]\tLoss: 1.786683\n",
            "Train (epoch 4/30) [5/20 (25%)]\tLoss: 1.882560\n",
            "Train (epoch 4/30) [10/20 (50%)]\tLoss: 1.581595\n",
            "Train (epoch 4/30) [15/20 (75%)]\tLoss: 1.785012\n",
            "Confusion Matrix:\n",
            "[[ 24 465 183   6  63   0   0   1   0 258]\n",
            " [  3 590 202  10  19   0   0  15   0 161]\n",
            " [  5 196 462  11 210   0   1  11   0 104]\n",
            " [  3 261 467  29 130   0   1  17   0  92]\n",
            " [  0  85 562  21 248   0   0  24   0  60]\n",
            " [  0 351 414  20 143   0   0  12   0  60]\n",
            " [  2  97 584  12 232   0   0  16   0  57]\n",
            " [  2 291 394  12  84   0   0  14   0 203]\n",
            " [  4 387 216   8  62   0   0   5   0 318]\n",
            " [  1 440 173   9  17   0   0   4   0 356]]\n",
            "TRAIN_LOSS:  1.686 TRAIN_ACC:  42.000\n",
            "VAL_LOSS:  2.305 VAL_ACC:  17.230\n",
            "Train (epoch 5/30) [0/20 (0%)]\tLoss: 1.861728\n",
            "Train (epoch 5/30) [5/20 (25%)]\tLoss: 1.702431\n",
            "Train (epoch 5/30) [10/20 (50%)]\tLoss: 0.967783\n",
            "Train (epoch 5/30) [15/20 (75%)]\tLoss: 1.270049\n",
            "Confusion Matrix:\n",
            "[[ 23 440 205   3  70   0   0   1   0 258]\n",
            " [  3 553 242   8  21   0   0  18   0 155]\n",
            " [  5 185 487   4 206   0   1  10   0 102]\n",
            " [  3 232 516  14 130   0   0  18   0  87]\n",
            " [  0  78 600   9 235   0   0  23   0  55]\n",
            " [  0 321 446  21 144   0   0  14   0  54]\n",
            " [  2  82 615   5 228   0   0  15   0  53]\n",
            " [  2 272 433   9  82   0   0  14   0 188]\n",
            " [  3 368 235   6  66   0   0   4   0 318]\n",
            " [  1 419 210   3  18   0   0   3   0 346]]\n",
            "TRAIN_LOSS:  1.656 TRAIN_ACC:  47.000\n",
            "VAL_LOSS:  2.306 VAL_ACC:  16.720\n",
            "Train (epoch 6/30) [0/20 (0%)]\tLoss: 1.867456\n",
            "Train (epoch 6/30) [5/20 (25%)]\tLoss: 2.099224\n",
            "Train (epoch 6/30) [10/20 (50%)]\tLoss: 1.559780\n",
            "Train (epoch 6/30) [15/20 (75%)]\tLoss: 1.976570\n",
            "Confusion Matrix:\n",
            "[[ 34 367 201   4  79   0   0   1   0 314]\n",
            " [  2 496 247   8  28   0   0  19   0 200]\n",
            " [  7 156 479   6 225   0   1  10   0 116]\n",
            " [  4 201 497  18 155   0   0  18   0 107]\n",
            " [  0  63 581   9 262   0   0  21   0  64]\n",
            " [  1 291 450  25 158   0   0  13   0  62]\n",
            " [  1  70 605   6 242   0   0  16   0  60]\n",
            " [  2 241 425  11  99   0   0  14   0 208]\n",
            " [  6 309 237   7  71   0   0   4   0 366]\n",
            " [  2 364 217   6  20   0   0   1   0 390]]\n",
            "TRAIN_LOSS:  1.685 TRAIN_ACC:  40.000\n",
            "VAL_LOSS:  2.300 VAL_ACC:  16.930\n",
            "Train (epoch 7/30) [0/20 (0%)]\tLoss: 1.708994\n",
            "Train (epoch 7/30) [5/20 (25%)]\tLoss: 1.929733\n",
            "Train (epoch 7/30) [10/20 (50%)]\tLoss: 1.761038\n",
            "Train (epoch 7/30) [15/20 (75%)]\tLoss: 1.917526\n",
            "Confusion Matrix:\n",
            "[[ 21 472 168   7  69   0   0   1   0 262]\n",
            " [  2 597 193   9  20   0   0  15   0 164]\n",
            " [  4 204 444  11 218   0   1  11   0 107]\n",
            " [  2 269 444  28 143   0   1  21   0  92]\n",
            " [  0  89 540  15 266   0   0  25   0  65]\n",
            " [  0 347 409  22 149   0   0  12   0  61]\n",
            " [  2  99 575  10 238   0   0  16   0  60]\n",
            " [  2 295 376  10  95   0   0  13   0 209]\n",
            " [  2 397 205   6  65   0   0   4   0 321]\n",
            " [  0 449 162   6  17   0   0   3   0 363]]\n",
            "TRAIN_LOSS:  1.660 TRAIN_ACC:  46.000\n",
            "VAL_LOSS:  2.309 VAL_ACC:  17.320\n",
            "Train (epoch 8/30) [0/20 (0%)]\tLoss: 2.025833\n",
            "Train (epoch 8/30) [5/20 (25%)]\tLoss: 1.625982\n",
            "Train (epoch 8/30) [10/20 (50%)]\tLoss: 1.577628\n",
            "Train (epoch 8/30) [15/20 (75%)]\tLoss: 2.010584\n",
            "Confusion Matrix:\n",
            "[[ 26 429 207   5  62   0   0   1   0 270]\n",
            " [  2 549 244   8  18   0   0  15   0 164]\n",
            " [  5 183 496   5 199   0   1  11   0 100]\n",
            " [  4 226 518  15 126   0   0  16   0  95]\n",
            " [  1  75 610  10 229   0   0  19   0  56]\n",
            " [  0 307 465  20 141   0   0  11   0  56]\n",
            " [  1  78 628   5 219   0   0  13   0  56]\n",
            " [  2 265 439  10  79   0   0  12   0 193]\n",
            " [  4 354 239   7  57   0   0   3   0 336]\n",
            " [  1 411 211   3  14   0   0   1   0 359]]\n",
            "TRAIN_LOSS:  1.640 TRAIN_ACC:  48.000\n",
            "VAL_LOSS:  2.305 VAL_ACC:  16.860\n",
            "Train (epoch 9/30) [0/20 (0%)]\tLoss: 1.288590\n",
            "Train (epoch 9/30) [5/20 (25%)]\tLoss: 1.999832\n",
            "Train (epoch 9/30) [10/20 (50%)]\tLoss: 1.257201\n",
            "Train (epoch 9/30) [15/20 (75%)]\tLoss: 1.860051\n",
            "Confusion Matrix:\n",
            "[[ 24 408 203   7  86   0   0   1   0 271]\n",
            " [  1 530 244  16  28   0   0  20   0 161]\n",
            " [  5 171 476   8 230   0   1  14   0  95]\n",
            " [  2 222 489  22 158   0   0  19   0  88]\n",
            " [  0  72 576  14 265   0   1  20   0  52]\n",
            " [  0 297 452  29 156   0   0  11   0  55]\n",
            " [  2  80 599   7 241   0   0  16   0  55]\n",
            " [  1 262 428  15 102   0   0  15   0 177]\n",
            " [  2 344 244   8  81   0   0   4   0 317]\n",
            " [  1 401 221  13  21   0   0   3   0 340]]\n",
            "TRAIN_LOSS:  1.698 TRAIN_ACC:  47.000\n",
            "VAL_LOSS:  2.310 VAL_ACC:  16.720\n",
            "Train (epoch 10/30) [0/20 (0%)]\tLoss: 1.490226\n",
            "Train (epoch 10/30) [5/20 (25%)]\tLoss: 2.085773\n",
            "Train (epoch 10/30) [10/20 (50%)]\tLoss: 1.725922\n",
            "Train (epoch 10/30) [15/20 (75%)]\tLoss: 1.394048\n",
            "Confusion Matrix:\n",
            "[[ 20 401 201   7  80   0   0   1   0 290]\n",
            " [  1 514 252  10  28   0   0  20   0 175]\n",
            " [  4 167 477   7 229   0   2  13   0 101]\n",
            " [  2 220 493  22 154   0   0  18   0  91]\n",
            " [  0  69 566  14 273   0   0  20   0  58]\n",
            " [  0 296 452  25 155   0   0  12   0  60]\n",
            " [  1  75 602   7 244   0   0  15   0  56]\n",
            " [  1 256 442  10  95   0   0  15   0 181]\n",
            " [  2 335 241   9  70   0   0   3   0 340]\n",
            " [  1 393 218  10  21   0   0   3   0 354]]\n",
            "TRAIN_LOSS:  1.630 TRAIN_ACC:  46.000\n",
            "VAL_LOSS:  2.299 VAL_ACC:  16.750\n",
            "Train (epoch 11/30) [0/20 (0%)]\tLoss: 1.556081\n",
            "Train (epoch 11/30) [5/20 (25%)]\tLoss: 2.011647\n",
            "Train (epoch 11/30) [10/20 (50%)]\tLoss: 1.804978\n",
            "Train (epoch 11/30) [15/20 (75%)]\tLoss: 1.584241\n",
            "Confusion Matrix:\n",
            "[[ 22 488 163   6  77   0   0   0   0 244]\n",
            " [  1 610 190  10  29   0   0  11   0 149]\n",
            " [  6 199 437   9 243   0   1  11   0  94]\n",
            " [  3 253 449  29 167   0   0  11   0  88]\n",
            " [  2  87 534  15 289   0   0  19   0  54]\n",
            " [  0 342 405  27 162   0   0   7   0  57]\n",
            " [  2  94 564  10 263   0   0  13   0  54]\n",
            " [  2 301 382  11 103   0   0  10   0 191]\n",
            " [  3 408 202   7  73   0   0   3   0 304]\n",
            " [  1 453 177  10  19   0   0   1   0 339]]\n",
            "TRAIN_LOSS:  1.674 TRAIN_ACC:  42.000\n",
            "VAL_LOSS:  2.305 VAL_ACC:  17.360\n",
            "Train (epoch 12/30) [0/20 (0%)]\tLoss: 2.195123\n",
            "Train (epoch 12/30) [5/20 (25%)]\tLoss: 1.742745\n",
            "Train (epoch 12/30) [10/20 (50%)]\tLoss: 1.594601\n",
            "Train (epoch 12/30) [15/20 (75%)]\tLoss: 1.521309\n",
            "Confusion Matrix:\n",
            "[[ 19 446 218   6  73   0   0   1   0 237]\n",
            " [  0 563 257   8  19   0   0  17   0 136]\n",
            " [  2 184 525   5 186   0   1  13   0  84]\n",
            " [  2 230 539  18 116   0   0  19   0  76]\n",
            " [  0  77 642  12 203   0   0  21   0  45]\n",
            " [  0 302 490  18 129   0   0  11   0  50]\n",
            " [  0  85 656   7 188   0   0  14   0  50]\n",
            " [  2 269 477  10  66   0   0  12   0 164]\n",
            " [  2 384 261   6  61   0   0   3   0 283]\n",
            " [  1 422 236   5  15   0   0   5   0 316]]\n",
            "TRAIN_LOSS:  1.694 TRAIN_ACC:  43.000\n",
            "VAL_LOSS:  2.314 VAL_ACC:  16.560\n",
            "Train (epoch 13/30) [0/20 (0%)]\tLoss: 1.495551\n",
            "Train (epoch 13/30) [5/20 (25%)]\tLoss: 1.621003\n",
            "Train (epoch 13/30) [10/20 (50%)]\tLoss: 2.157588\n",
            "Train (epoch 13/30) [15/20 (75%)]\tLoss: 1.618797\n",
            "Confusion Matrix:\n",
            "[[ 23 494 172   3  63   0   0   0   0 245]\n",
            " [  3 622 179   9  21   0   0  12   0 154]\n",
            " [  7 216 451  10 207   0   1  12   0  96]\n",
            " [  3 284 444  24 133   0   0  18   0  94]\n",
            " [  0  98 560  15 243   0   0  24   0  60]\n",
            " [  0 375 392  18 144   0   0   9   0  62]\n",
            " [  2 110 582   9 229   0   0  16   0  52]\n",
            " [  2 315 373   8  89   0   0  12   0 201]\n",
            " [  4 419 200   6  61   0   0   4   0 306]\n",
            " [  0 473 156   6  15   0   0   3   0 347]]\n",
            "TRAIN_LOSS:  1.721 TRAIN_ACC:  45.000\n",
            "VAL_LOSS:  2.322 VAL_ACC:  17.220\n",
            "Train (epoch 14/30) [0/20 (0%)]\tLoss: 1.927505\n",
            "Train (epoch 14/30) [5/20 (25%)]\tLoss: 2.123574\n",
            "Train (epoch 14/30) [10/20 (50%)]\tLoss: 1.791526\n",
            "Train (epoch 14/30) [15/20 (75%)]\tLoss: 1.725168\n",
            "Confusion Matrix:\n",
            "[[ 24 465 180  11  68   0   0   1   0 251]\n",
            " [  0 619 195  11  18   0   0  11   0 146]\n",
            " [  4 207 463  12 204   0   1  13   0  96]\n",
            " [  3 279 463  30 127   0   0  15   0  83]\n",
            " [  0  93 568  25 241   0   0  22   0  51]\n",
            " [  0 360 405  24 143   0   0  10   0  58]\n",
            " [  2 113 588  12 218   0   0  15   0  52]\n",
            " [  2 308 389  12  87   0   0  12   0 190]\n",
            " [  2 407 215   7  68   0   0   4   0 297]\n",
            " [  1 482 167  14  16   0   0   4   0 316]]\n",
            "TRAIN_LOSS:  1.710 TRAIN_ACC:  41.000\n",
            "VAL_LOSS:  2.313 VAL_ACC:  17.050\n",
            "Train (epoch 15/30) [0/20 (0%)]\tLoss: 1.561801\n",
            "Train (epoch 15/30) [5/20 (25%)]\tLoss: 1.535761\n",
            "Train (epoch 15/30) [10/20 (50%)]\tLoss: 1.603242\n",
            "Train (epoch 15/30) [15/20 (75%)]\tLoss: 1.541100\n",
            "Confusion Matrix:\n",
            "[[ 22 513 163   5  55   0   0   0   0 242]\n",
            " [  3 654 168   6  10   0   0  10   0 149]\n",
            " [  7 225 457   6 195   0   1  12   0  97]\n",
            " [  2 304 451  26 110   0   0  17   0  90]\n",
            " [  2 109 577  16 216   0   0  20   0  60]\n",
            " [  0 393 400  19 118   0   0   8   0  62]\n",
            " [  2 127 588   9 207   0   0  14   0  53]\n",
            " [  2 335 376   9  64   0   0   9   0 205]\n",
            " [  3 445 205   7  46   0   0   4   0 290]\n",
            " [  1 500 143   4   9   0   0   3   0 340]]\n",
            "TRAIN_LOSS:  1.674 TRAIN_ACC:  49.000\n",
            "VAL_LOSS:  2.327 VAL_ACC:  17.240\n",
            "Train (epoch 16/30) [0/20 (0%)]\tLoss: 1.499962\n",
            "Train (epoch 16/30) [5/20 (25%)]\tLoss: 1.661750\n",
            "Train (epoch 16/30) [10/20 (50%)]\tLoss: 1.366881\n",
            "Train (epoch 16/30) [15/20 (75%)]\tLoss: 1.286523\n",
            "Confusion Matrix:\n",
            "[[ 22 444 177   7  61   0   0   1   0 288]\n",
            " [  1 555 208  11  18   0   0  16   0 191]\n",
            " [  6 183 465  13 207   0   1   8   0 117]\n",
            " [  4 231 479  32 131   0   0  16   0 107]\n",
            " [  0  76 567  18 249   0   0  23   0  67]\n",
            " [  0 325 421  29 143   0   0  12   0  70]\n",
            " [  1  82 601   8 228   0   0  16   0  64]\n",
            " [  1 268 406  10  81   0   0  13   0 221]\n",
            " [  3 360 218   6  55   0   0   4   0 354]\n",
            " [  1 400 172   8  14   0   0   2   0 403]]\n",
            "TRAIN_LOSS:  1.640 TRAIN_ACC:  45.000\n",
            "VAL_LOSS:  2.302 VAL_ACC:  17.390\n",
            "Train (epoch 17/30) [0/20 (0%)]\tLoss: 1.113437\n",
            "Train (epoch 17/30) [5/20 (25%)]\tLoss: 1.275453\n",
            "Train (epoch 17/30) [10/20 (50%)]\tLoss: 1.291301\n",
            "Train (epoch 17/30) [15/20 (75%)]\tLoss: 1.844508\n",
            "Confusion Matrix:\n",
            "[[ 33 415 194   3  59   0   0   1   0 295]\n",
            " [  4 543 214  10  19   0   0  14   0 196]\n",
            " [  8 183 467   9 206   0   1  11   0 115]\n",
            " [  3 233 486  26 121   0   0  18   0 113]\n",
            " [  0  78 579  13 239   0   0  22   0  69]\n",
            " [  2 329 432  19 134   0   0  12   0  72]\n",
            " [  3  82 608   4 223   0   0  17   0  63]\n",
            " [  1 273 411   7  73   0   0  13   0 222]\n",
            " [  6 344 227   6  53   0   0   4   0 360]\n",
            " [  2 397 179   6  13   0   0   3   0 400]]\n",
            "TRAIN_LOSS:  1.652 TRAIN_ACC:  40.000\n",
            "VAL_LOSS:  2.305 VAL_ACC:  17.210\n",
            "Train (epoch 18/30) [0/20 (0%)]\tLoss: 1.586134\n",
            "Train (epoch 18/30) [5/20 (25%)]\tLoss: 1.323945\n",
            "Train (epoch 18/30) [10/20 (50%)]\tLoss: 1.791964\n",
            "Train (epoch 18/30) [15/20 (75%)]\tLoss: 1.341385\n",
            "Confusion Matrix:\n",
            "[[ 30 470 178   4  67   0   0   0   0 251]\n",
            " [  3 603 194   9  24   0   0  11   0 156]\n",
            " [  7 203 446   9 222   0   1   9   0 103]\n",
            " [  4 268 455  29 145   0   0  12   0  87]\n",
            " [  0  92 546  15 270   0   0  20   0  57]\n",
            " [  1 354 404  20 151   0   0   8   0  62]\n",
            " [  2 101 575  11 241   0   0  13   0  57]\n",
            " [  2 300 384  11  93   0   0  11   0 199]\n",
            " [  6 396 213   4  65   0   0   3   0 313]\n",
            " [  2 448 172   9  16   0   0   2   0 351]]\n",
            "TRAIN_LOSS:  1.696 TRAIN_ACC:  46.000\n",
            "VAL_LOSS:  2.309 VAL_ACC:  17.400\n",
            "Train (epoch 19/30) [0/20 (0%)]\tLoss: 1.856555\n",
            "Train (epoch 19/30) [5/20 (25%)]\tLoss: 1.829670\n",
            "Train (epoch 19/30) [10/20 (50%)]\tLoss: 1.778332\n",
            "Train (epoch 19/30) [15/20 (75%)]\tLoss: 1.929245\n",
            "Confusion Matrix:\n",
            "[[ 27 382 209   5  71   0   0   0   0 306]\n",
            " [  1 494 248  15  23   0   0  16   0 203]\n",
            " [  6 159 488   6 220   0   1   7   0 113]\n",
            " [  2 218 500  20 145   0   0  11   0 104]\n",
            " [  0  73 582   9 260   0   0  18   0  58]\n",
            " [  1 299 450  25 156   0   0   8   0  61]\n",
            " [  2  76 603   6 241   0   0  13   0  59]\n",
            " [  1 250 422  10  99   0   0  10   0 208]\n",
            " [  5 326 241   9  70   0   0   1   0 348]\n",
            " [  3 376 212   7  18   0   0   2   0 382]]\n",
            "TRAIN_LOSS:  1.727 TRAIN_ACC:  42.000\n",
            "VAL_LOSS:  2.305 VAL_ACC:  16.810\n",
            "Train (epoch 20/30) [0/20 (0%)]\tLoss: 1.811281\n",
            "Train (epoch 20/30) [5/20 (25%)]\tLoss: 1.757333\n",
            "Train (epoch 20/30) [10/20 (50%)]\tLoss: 1.663033\n",
            "Train (epoch 20/30) [15/20 (75%)]\tLoss: 1.992994\n",
            "Confusion Matrix:\n",
            "[[ 29 420 187   7  84   0   0   1   0 272]\n",
            " [  1 534 229  15  27   0   0  17   0 177]\n",
            " [  5 173 459   7 238   0   1  13   0 104]\n",
            " [  3 223 477  22 165   0   0  18   0  92]\n",
            " [  1  74 555  14 280   0   0  20   0  56]\n",
            " [  0 308 434  28 160   0   0  11   0  59]\n",
            " [  1  82 586   9 252   0   0  15   0  55]\n",
            " [  2 263 405  15 108   0   0  12   0 195]\n",
            " [  4 341 226   7  76   0   0   5   0 341]\n",
            " [  1 401 206   7  22   0   0   1   0 362]]\n",
            "TRAIN_LOSS:  1.667 TRAIN_ACC:  41.000\n",
            "VAL_LOSS:  2.303 VAL_ACC:  16.980\n",
            "Train (epoch 21/30) [0/20 (0%)]\tLoss: 1.270709\n",
            "Train (epoch 21/30) [5/20 (25%)]\tLoss: 1.445186\n",
            "Train (epoch 21/30) [10/20 (50%)]\tLoss: 1.529974\n",
            "Train (epoch 21/30) [15/20 (75%)]\tLoss: 1.846066\n",
            "Confusion Matrix:\n",
            "[[ 22 456 193   5  76   0   0   1   0 247]\n",
            " [  1 565 234  14  24   0   0  11   0 151]\n",
            " [  2 190 474   9 218   0   1   6   0 100]\n",
            " [  3 238 497  24 143   0   0  11   0  84]\n",
            " [  0  83 591  16 243   0   0  18   0  49]\n",
            " [  0 319 443  28 148   0   0   8   0  54]\n",
            " [  1  85 612   8 230   0   0  12   0  52]\n",
            " [  2 277 420  13  95   0   0   7   0 186]\n",
            " [  3 375 232   7  69   0   0   2   0 312]\n",
            " [  1 427 208   9  18   0   0   1   0 336]]\n",
            "TRAIN_LOSS:  1.702 TRAIN_ACC:  47.000\n",
            "VAL_LOSS:  2.312 VAL_ACC:  16.710\n",
            "Train (epoch 22/30) [0/20 (0%)]\tLoss: 1.967563\n",
            "Train (epoch 22/30) [5/20 (25%)]\tLoss: 2.124876\n",
            "Train (epoch 22/30) [10/20 (50%)]\tLoss: 1.895943\n",
            "Train (epoch 22/30) [15/20 (75%)]\tLoss: 1.496806\n",
            "Confusion Matrix:\n",
            "[[ 31 419 192   5  78   0   0   1   0 274]\n",
            " [  3 547 226  10  29   0   0  17   0 168]\n",
            " [  5 177 463  13 223   0   2  12   0 105]\n",
            " [  4 227 473  34 152   0   0  17   0  93]\n",
            " [  0  71 566  19 263   0   0  23   0  58]\n",
            " [  1 307 435  29 154   0   0  12   0  62]\n",
            " [  1  77 591  12 247   0   0  15   0  57]\n",
            " [  2 262 414  12 100   0   0  15   0 195]\n",
            " [  4 352 224   9  68   0   0   4   0 339]\n",
            " [  1 406 196  11  18   0   0   3   0 365]]\n",
            "TRAIN_LOSS:  1.698 TRAIN_ACC:  43.000\n",
            "VAL_LOSS:  2.296 VAL_ACC:  17.180\n",
            "Train (epoch 23/30) [0/20 (0%)]\tLoss: 1.526303\n",
            "Train (epoch 23/30) [5/20 (25%)]\tLoss: 1.469681\n",
            "Train (epoch 23/30) [10/20 (50%)]\tLoss: 1.583618\n",
            "Train (epoch 23/30) [15/20 (75%)]\tLoss: 2.046558\n",
            "Confusion Matrix:\n",
            "[[ 26 435 202   3  71   0   0   4   0 259]\n",
            " [  2 554 240   9  18   0   0  19   0 158]\n",
            " [  5 192 483   5 206   0   1  13   0  95]\n",
            " [  4 235 509  17 127   0   0  18   0  90]\n",
            " [  0  77 603  12 229   0   0  23   0  56]\n",
            " [  0 320 449  20 141   0   0  13   0  57]\n",
            " [  2  87 619   5 217   0   0  17   0  53]\n",
            " [  1 278 421   7  79   0   0  15   0 199]\n",
            " [  4 361 236   5  65   0   0   4   0 325]\n",
            " [  1 422 201   5  15   0   0   3   0 353]]\n",
            "TRAIN_LOSS:  1.642 TRAIN_ACC:  46.000\n",
            "VAL_LOSS:  2.308 VAL_ACC:  16.770\n",
            "Train (epoch 24/30) [0/20 (0%)]\tLoss: 1.747585\n",
            "Train (epoch 24/30) [5/20 (25%)]\tLoss: 1.280629\n",
            "Train (epoch 24/30) [10/20 (50%)]\tLoss: 1.851054\n",
            "Train (epoch 24/30) [15/20 (75%)]\tLoss: 1.985811\n",
            "Confusion Matrix:\n",
            "[[ 15 478 200   6  76   0   0   2   0 223]\n",
            " [  0 587 226  14  22   0   0  19   0 132]\n",
            " [  2 198 481   9 210   0   1  13   0  86]\n",
            " [  1 252 499  26 133   0   0  14   0  75]\n",
            " [  0  91 598  16 235   0   0  22   0  38]\n",
            " [  0 334 436  28 141   0   0  11   0  50]\n",
            " [  1 103 614   7 216   0   0  12   0  47]\n",
            " [  1 296 426  14  85   0   0  11   0 167]\n",
            " [  2 399 234  10  71   0   0   3   0 281]\n",
            " [  0 461 208  12  17   0   0   3   0 299]]\n",
            "TRAIN_LOSS:  1.648 TRAIN_ACC:  43.000\n",
            "VAL_LOSS:  2.314 VAL_ACC:  16.540\n",
            "Train (epoch 25/30) [0/20 (0%)]\tLoss: 1.680817\n",
            "Train (epoch 25/30) [5/20 (25%)]\tLoss: 2.563602\n",
            "Train (epoch 25/30) [10/20 (50%)]\tLoss: 1.302781\n",
            "Train (epoch 25/30) [15/20 (75%)]\tLoss: 1.169437\n",
            "Confusion Matrix:\n",
            "[[ 30 431 193  10  63   0   0   1   0 272]\n",
            " [  2 571 213  11  16   0   0  17   0 170]\n",
            " [  6 180 478  10 203   0   1  12   0 110]\n",
            " [  3 242 486  30 122   0   0  18   0  99]\n",
            " [  0  81 588  16 231   0   0  23   0  61]\n",
            " [  0 331 429  29 137   0   0  13   0  61]\n",
            " [  2  90 598  12 221   0   0  17   0  60]\n",
            " [  2 279 409  14  80   0   0  16   0 200]\n",
            " [  5 368 221   9  62   0   0   4   0 331]\n",
            " [  1 429 178  10  17   0   0   4   0 361]]\n",
            "TRAIN_LOSS:  1.656 TRAIN_ACC:  42.000\n",
            "VAL_LOSS:  2.303 VAL_ACC:  17.170\n",
            "Train (epoch 26/30) [0/20 (0%)]\tLoss: 1.917920\n",
            "Train (epoch 26/30) [5/20 (25%)]\tLoss: 1.659887\n",
            "Train (epoch 26/30) [10/20 (50%)]\tLoss: 1.873775\n",
            "Train (epoch 26/30) [15/20 (75%)]\tLoss: 1.512852\n",
            "Confusion Matrix:\n",
            "[[ 24 408 195   5  75   0   0   0   0 293]\n",
            " [  1 516 242  11  22   0   0  17   0 191]\n",
            " [  5 163 481   6 221   0   1   7   0 116]\n",
            " [  3 218 514  15 140   0   0  11   0  99]\n",
            " [  0  72 591   8 252   0   0  19   0  58]\n",
            " [  0 297 460  23 149   0   0   8   0  63]\n",
            " [  1  76 614   7 230   0   0  13   0  59]\n",
            " [  0 254 429  11  91   0   0   9   0 206]\n",
            " [  3 338 234   7  67   0   0   1   0 350]\n",
            " [  1 389 207   5  19   0   0   2   0 377]]\n",
            "TRAIN_LOSS:  1.653 TRAIN_ACC:  44.000\n",
            "VAL_LOSS:  2.305 VAL_ACC:  16.740\n",
            "Train (epoch 27/30) [0/20 (0%)]\tLoss: 1.529201\n",
            "Train (epoch 27/30) [5/20 (25%)]\tLoss: 1.067127\n",
            "Train (epoch 27/30) [10/20 (50%)]\tLoss: 1.577445\n",
            "Train (epoch 27/30) [15/20 (75%)]\tLoss: 1.675723\n",
            "Confusion Matrix:\n",
            "[[ 14 549 163   2  63   0   0   0   0 209]\n",
            " [  3 672 172   4  19   0   0  10   0 120]\n",
            " [  5 234 451   3 214   0   1   8   0  84]\n",
            " [  4 301 447  15 138   0   0  11   0  84]\n",
            " [  2 113 543   7 262   0   0  20   0  53]\n",
            " [  0 397 386  11 139   0   0   7   0  60]\n",
            " [  3 123 569   4 241   0   0  10   0  50]\n",
            " [  3 353 359   4  84   0   0   9   0 188]\n",
            " [  5 466 199   1  61   0   0   3   0 265]\n",
            " [  1 506 152   0  16   0   0   2   0 323]]\n",
            "TRAIN_LOSS:  1.682 TRAIN_ACC:  43.000\n",
            "VAL_LOSS:  2.333 VAL_ACC:  17.460\n",
            "Train (epoch 28/30) [0/20 (0%)]\tLoss: 1.447011\n",
            "Train (epoch 28/30) [5/20 (25%)]\tLoss: 1.348471\n",
            "Train (epoch 28/30) [10/20 (50%)]\tLoss: 2.144272\n",
            "Train (epoch 28/30) [15/20 (75%)]\tLoss: 1.598670\n",
            "Confusion Matrix:\n",
            "[[ 25 493 151   6  68   0   0   0   0 257]\n",
            " [  1 636 171   7  20   0   0  11   0 154]\n",
            " [  6 212 439   8 220   0   1  10   0 104]\n",
            " [  3 280 437  30 145   0   1  12   0  92]\n",
            " [  2  97 531  18 269   0   0  22   0  61]\n",
            " [  0 373 391  18 149   0   0   7   0  62]\n",
            " [  2 113 566  10 241   0   0  15   0  53]\n",
            " [  2 315 360  11  94   0   0  11   0 207]\n",
            " [  3 421 197   2  64   0   0   3   0 310]\n",
            " [  1 471 147   7  17   0   0   2   0 355]]\n",
            "TRAIN_LOSS:  1.693 TRAIN_ACC:  46.000\n",
            "VAL_LOSS:  2.308 VAL_ACC:  17.650\n",
            "Train (epoch 29/30) [0/20 (0%)]\tLoss: 1.236004\n",
            "Train (epoch 29/30) [5/20 (25%)]\tLoss: 1.745391\n",
            "Train (epoch 29/30) [10/20 (50%)]\tLoss: 1.560229\n",
            "Train (epoch 29/30) [15/20 (75%)]\tLoss: 1.505730\n",
            "Confusion Matrix:\n",
            "[[ 23 513 144   7  63   0   0   0   0 250]\n",
            " [  1 659 153   6  19   0   0  11   0 151]\n",
            " [  6 222 424  11 224   0   0  10   0 103]\n",
            " [  3 291 420  29 147   0   0  10   0 100]\n",
            " [  2 101 510  17 279   0   0  21   0  70]\n",
            " [  0 394 366  21 146   0   0   8   0  65]\n",
            " [  1 121 538  13 254   0   0  15   0  58]\n",
            " [  2 341 342  11  88   0   0  10   0 206]\n",
            " [  4 449 186   3  60   0   0   2   0 296]\n",
            " [  0 502 133   6  14   0   0   2   0 343]]\n",
            "TRAIN_LOSS:  1.708 TRAIN_ACC:  40.000\n",
            "VAL_LOSS:  2.311 VAL_ACC:  17.670\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc9JNyaN4k8G"
      },
      "source": [
        "From Scratch:\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "| Model | Number of  epochs  | Train accuracy | Test accuracy |\r\n",
        "|------|------|------|------|\r\n",
        "|   Resnet18 | 30(best test acc got on epoch=18) | 40.00 | 17.67 |\r\n",
        "\r\n",
        "---\r\n",
        "Accuracy for the whole dataset:\r\n",
        "\r\n",
        "\r\n",
        "*   link:https://github.com/kuangliu/pytorch-cifar\r\n",
        "\r\n",
        "---\r\n",
        "| Model |  accuracy | \r\n",
        "|------|------\r\n",
        "|   Resnet18 |93.02  |\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eOWeo0i6mda"
      },
      "source": [
        "# Transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxMMaa_f6mda"
      },
      "source": [
        "We propose to use pre-trained models on a classification and generative task, in order to improve the results of our setting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBq3RB0K6mda"
      },
      "source": [
        "## ImageNet features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abU5etU-6mda"
      },
      "source": [
        "Now, we will use some pre-trained models on ImageNet and see how well they compare on CIFAR. A list is available on: https://pytorch.org/docs/stable/torchvision/models.html.\n",
        "\n",
        "__Question 4 (3 points):__ Pick a model from the list above, adapt it for CIFAR and retrain its final layer (or a block of layers, depending on the resources to which you have access to). Report its accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_56cTaBM6mda"
      },
      "source": [
        "def train_model(model, criterion, optimizer, num_epochs=30):\r\n",
        "    since = time.time()\r\n",
        "    model.train()\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\r\n",
        "        print('-' * 10)\r\n",
        "\r\n",
        "        running_loss = 0.0\r\n",
        "        running_corrects = 0\r\n",
        "\r\n",
        "        # Iterate over data.\r\n",
        "        for inputs, labels in train_batch_dataloader: \r\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\r\n",
        "            optimizer.zero_grad()\r\n",
        "            outputs = model(inputs)\r\n",
        "            _, preds = torch.max(outputs, 1)\r\n",
        "            loss = criterion(outputs, labels)\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "\r\n",
        "            # statistics\r\n",
        "            running_loss += loss.item() * inputs.size(0)\r\n",
        "            running_corrects += torch.sum(preds == labels.data)\r\n",
        "           \r\n",
        "        epoch_loss = running_loss / 100\r\n",
        "        epoch_acc = running_corrects.double() / 100\r\n",
        "\r\n",
        "        print('Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\r\n",
        "    print('Time for training is {}'.format(time.time()-since))"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxrHhmXfLE3Y"
      },
      "source": [
        "--------------------------------------------------------DenseNet121-----------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utTFHwVHLE3Y"
      },
      "source": [
        "# Densenet121\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import copy\n",
        "model_conv = torchvision.models.densenet121(pretrained=True).to(device)\n",
        "for param in model_conv.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "num_ftrs = model_conv.classifier.in_features\n",
        "model_conv.classifier = nn.Linear(num_ftrs, 10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_conv = optim.SGD(model_conv.classifier.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS3S1dcTLE3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1345f98-bd3e-4fab-fd89-2ed7d2cfd17e"
      },
      "source": [
        "train_model(model_conv, criterion, optimizer_conv, num_epochs=30)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "----------\n",
            "Loss: 2.3300 Acc: 0.1200\n",
            "Epoch 2/30\n",
            "----------\n",
            "Loss: 2.3024 Acc: 0.1700\n",
            "Epoch 3/30\n",
            "----------\n",
            "Loss: 2.0590 Acc: 0.2400\n",
            "Epoch 4/30\n",
            "----------\n",
            "Loss: 1.9876 Acc: 0.3000\n",
            "Epoch 5/30\n",
            "----------\n",
            "Loss: 1.8210 Acc: 0.3600\n",
            "Epoch 6/30\n",
            "----------\n",
            "Loss: 1.8457 Acc: 0.3400\n",
            "Epoch 7/30\n",
            "----------\n",
            "Loss: 1.7019 Acc: 0.3900\n",
            "Epoch 8/30\n",
            "----------\n",
            "Loss: 1.7176 Acc: 0.4300\n",
            "Epoch 9/30\n",
            "----------\n",
            "Loss: 1.6580 Acc: 0.4700\n",
            "Epoch 10/30\n",
            "----------\n",
            "Loss: 1.5837 Acc: 0.4900\n",
            "Epoch 11/30\n",
            "----------\n",
            "Loss: 1.3281 Acc: 0.5800\n",
            "Epoch 12/30\n",
            "----------\n",
            "Loss: 1.4148 Acc: 0.5700\n",
            "Epoch 13/30\n",
            "----------\n",
            "Loss: 1.5563 Acc: 0.4800\n",
            "Epoch 14/30\n",
            "----------\n",
            "Loss: 1.2131 Acc: 0.6200\n",
            "Epoch 15/30\n",
            "----------\n",
            "Loss: 1.3908 Acc: 0.5300\n",
            "Epoch 16/30\n",
            "----------\n",
            "Loss: 1.5788 Acc: 0.4400\n",
            "Epoch 17/30\n",
            "----------\n",
            "Loss: 1.3897 Acc: 0.5400\n",
            "Epoch 18/30\n",
            "----------\n",
            "Loss: 1.4053 Acc: 0.5400\n",
            "Epoch 19/30\n",
            "----------\n",
            "Loss: 1.2621 Acc: 0.5500\n",
            "Epoch 20/30\n",
            "----------\n",
            "Loss: 1.2389 Acc: 0.6200\n",
            "Epoch 21/30\n",
            "----------\n",
            "Loss: 1.3441 Acc: 0.5700\n",
            "Epoch 22/30\n",
            "----------\n",
            "Loss: 1.2624 Acc: 0.6400\n",
            "Epoch 23/30\n",
            "----------\n",
            "Loss: 1.3630 Acc: 0.5800\n",
            "Epoch 24/30\n",
            "----------\n",
            "Loss: 1.3703 Acc: 0.5500\n",
            "Epoch 25/30\n",
            "----------\n",
            "Loss: 1.1023 Acc: 0.6100\n",
            "Epoch 26/30\n",
            "----------\n",
            "Loss: 1.0807 Acc: 0.6300\n",
            "Epoch 27/30\n",
            "----------\n",
            "Loss: 1.1877 Acc: 0.6100\n",
            "Epoch 28/30\n",
            "----------\n",
            "Loss: 1.1428 Acc: 0.6500\n",
            "Epoch 29/30\n",
            "----------\n",
            "Loss: 1.0820 Acc: 0.6400\n",
            "Epoch 30/30\n",
            "----------\n",
            "Loss: 1.3000 Acc: 0.5700\n",
            "Time for training is 13.254425048828125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBqcwEfTLE3Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50da913a-ab83-42eb-9515-3c23e7e96a2a"
      },
      "source": [
        "model_conv.eval()\n",
        "start = time.time()\n",
        "with torch.no_grad():\n",
        "    corrects = 0.0\n",
        "    for images, labels in val_batch_dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model_conv(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "print('Test accuracy is {}'.format(100 * corrects.double() / len(testset)))        \n",
        "print('Time for test is {}'.format(time.time()-start))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy is 21.540000000000003\n",
            "Time for test is 31.25330114364624\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qikvCf9LE3Z"
      },
      "source": [
        "----------------------------------------------DenseNet121 + Same preprocess---------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlIRVwiXLE3Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5288676a-eac6-4d09-da90-8f045dffbc8f"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "test_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "sample = list(range(0,100))\n",
        "subset = torch.utils.data.Subset(trainset,sample)\n",
        "train_batch_dataloader = torch.utils.data.DataLoader(subset, batch_size=4, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "test_batch_dataloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qm9410J2LE3a"
      },
      "source": [
        "# Densenet121\n",
        "model_conv = torchvision.models.densenet121(pretrained=True).to(device)\n",
        "for param in model_conv.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "num_ftrs = model_conv.classifier.in_features\n",
        "model_conv.classifier = nn.Linear(num_ftrs, 10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_conv = optim.SGD(model_conv.classifier.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CRwpbQhLE3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c98220c6-9569-404a-8ae3-84572e85427e"
      },
      "source": [
        "train_model(model_conv, criterion, optimizer_conv,num_epochs=30)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "----------\n",
            "Loss: 2.3523 Acc: 0.1200\n",
            "Epoch 2/30\n",
            "----------\n",
            "Loss: 2.3117 Acc: 0.2100\n",
            "Epoch 3/30\n",
            "----------\n",
            "Loss: 2.1713 Acc: 0.2400\n",
            "Epoch 4/30\n",
            "----------\n",
            "Loss: 1.9603 Acc: 0.2900\n",
            "Epoch 5/30\n",
            "----------\n",
            "Loss: 2.0326 Acc: 0.3300\n",
            "Epoch 6/30\n",
            "----------\n",
            "Loss: 1.9665 Acc: 0.3500\n",
            "Epoch 7/30\n",
            "----------\n",
            "Loss: 1.8633 Acc: 0.3300\n",
            "Epoch 8/30\n",
            "----------\n",
            "Loss: 1.8232 Acc: 0.4000\n",
            "Epoch 9/30\n",
            "----------\n",
            "Loss: 1.7936 Acc: 0.3300\n",
            "Epoch 10/30\n",
            "----------\n",
            "Loss: 1.8154 Acc: 0.4000\n",
            "Epoch 11/30\n",
            "----------\n",
            "Loss: 1.7146 Acc: 0.4300\n",
            "Epoch 12/30\n",
            "----------\n",
            "Loss: 1.7153 Acc: 0.3600\n",
            "Epoch 13/30\n",
            "----------\n",
            "Loss: 1.7479 Acc: 0.4200\n",
            "Epoch 14/30\n",
            "----------\n",
            "Loss: 1.5621 Acc: 0.4900\n",
            "Epoch 15/30\n",
            "----------\n",
            "Loss: 1.6656 Acc: 0.4000\n",
            "Epoch 16/30\n",
            "----------\n",
            "Loss: 1.7217 Acc: 0.4100\n",
            "Epoch 17/30\n",
            "----------\n",
            "Loss: 1.4663 Acc: 0.5100\n",
            "Epoch 18/30\n",
            "----------\n",
            "Loss: 1.5564 Acc: 0.4700\n",
            "Epoch 19/30\n",
            "----------\n",
            "Loss: 1.3811 Acc: 0.5200\n",
            "Epoch 20/30\n",
            "----------\n",
            "Loss: 1.4121 Acc: 0.5000\n",
            "Epoch 21/30\n",
            "----------\n",
            "Loss: 1.3911 Acc: 0.5900\n",
            "Epoch 22/30\n",
            "----------\n",
            "Loss: 1.3678 Acc: 0.5300\n",
            "Epoch 23/30\n",
            "----------\n",
            "Loss: 1.3454 Acc: 0.5300\n",
            "Epoch 24/30\n",
            "----------\n",
            "Loss: 1.4617 Acc: 0.4900\n",
            "Epoch 25/30\n",
            "----------\n",
            "Loss: 1.3420 Acc: 0.5400\n",
            "Epoch 26/30\n",
            "----------\n",
            "Loss: 1.2406 Acc: 0.5200\n",
            "Epoch 27/30\n",
            "----------\n",
            "Loss: 1.3504 Acc: 0.5500\n",
            "Epoch 28/30\n",
            "----------\n",
            "Loss: 1.3435 Acc: 0.5400\n",
            "Epoch 29/30\n",
            "----------\n",
            "Loss: 1.3020 Acc: 0.5700\n",
            "Epoch 30/30\n",
            "----------\n",
            "Loss: 1.2658 Acc: 0.5400\n",
            "Time for training is 21.121127128601074\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV8WcKZ1LE3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5814cf1b-5e7a-484c-f4cb-234bb62bcc17"
      },
      "source": [
        "model_conv.eval()\n",
        "start = time.time()\n",
        "with torch.no_grad():\n",
        "    corrects = 0.0\n",
        "    for images, labels in test_batch_dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model_conv(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "print('Test accuracy is {}'.format(100 * corrects.double() / len(testset)))        \n",
        "print('Time for test is {}'.format(time.time()-start))"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy is 50.6\n",
            "Time for test is 54.011399269104004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5tILGQ1LE3b"
      },
      "source": [
        "----------------------------------------------Resnet18 + Same preprocess---------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kki-82u9LE3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd1d05fc-a04b-421e-a5c8-65b1fa76ab06"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "test_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "sample = list(range(0,100))\n",
        "subset = torch.utils.data.Subset(trainset,sample)\n",
        "train_batch_dataloader = torch.utils.data.DataLoader(subset, batch_size=4, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "test_batch_dataloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT01_bsQLE3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "2202d0a14c1d43cc8bbcd3269bdb5845",
            "29adc4b2f92448d09f4502c18b1f8687",
            "dc6b9d589b644fd081bd04eda0adce48",
            "748fbf17fced427b8c0a1f28a0990a4a",
            "6e1d48ce2c094ec3a57d8b08f7b94968",
            "bfec5c1c49ba4e4c82fbd39af81e667c",
            "26fdfa3de1e14759aa477c2dfb67b7f3",
            "8c2bdc94eb794416be9b83190f62fe00"
          ]
        },
        "outputId": "59d7f287-ba45-43b6-be47-ea0e58b4bbca"
      },
      "source": [
        "# Resnet18\n",
        "pre_resnet = torchvision.models.resnet18(pretrained=True).to(device)\n",
        "for param in pre_resnet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "num_ftrs = pre_resnet.fc.in_features\n",
        "pre_resnet.fc = nn.Linear(num_ftrs, 10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_conv = optim.SGD(pre_resnet.fc.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2202d0a14c1d43cc8bbcd3269bdb5845",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X26DUBVLE3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6803983-ed8a-4bb6-ba60-d0cb235a14d3"
      },
      "source": [
        "train_model(pre_resnet, criterion, optimizer_conv, num_epochs=30)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "----------\n",
            "Loss: 2.5426 Acc: 0.1100\n",
            "Epoch 2/30\n",
            "----------\n",
            "Loss: 2.3251 Acc: 0.1900\n",
            "Epoch 3/30\n",
            "----------\n",
            "Loss: 2.2991 Acc: 0.2300\n",
            "Epoch 4/30\n",
            "----------\n",
            "Loss: 2.1176 Acc: 0.2500\n",
            "Epoch 5/30\n",
            "----------\n",
            "Loss: 1.9982 Acc: 0.3000\n",
            "Epoch 6/30\n",
            "----------\n",
            "Loss: 2.0876 Acc: 0.2500\n",
            "Epoch 7/30\n",
            "----------\n",
            "Loss: 1.9667 Acc: 0.3500\n",
            "Epoch 8/30\n",
            "----------\n",
            "Loss: 1.9093 Acc: 0.3400\n",
            "Epoch 9/30\n",
            "----------\n",
            "Loss: 1.8286 Acc: 0.3100\n",
            "Epoch 10/30\n",
            "----------\n",
            "Loss: 1.6987 Acc: 0.4700\n",
            "Epoch 11/30\n",
            "----------\n",
            "Loss: 1.7230 Acc: 0.5000\n",
            "Epoch 12/30\n",
            "----------\n",
            "Loss: 1.6933 Acc: 0.4200\n",
            "Epoch 13/30\n",
            "----------\n",
            "Loss: 1.6142 Acc: 0.4500\n",
            "Epoch 14/30\n",
            "----------\n",
            "Loss: 1.7159 Acc: 0.4100\n",
            "Epoch 15/30\n",
            "----------\n",
            "Loss: 1.5982 Acc: 0.4700\n",
            "Epoch 16/30\n",
            "----------\n",
            "Loss: 1.6501 Acc: 0.5300\n",
            "Epoch 17/30\n",
            "----------\n",
            "Loss: 1.5845 Acc: 0.5500\n",
            "Epoch 18/30\n",
            "----------\n",
            "Loss: 1.5460 Acc: 0.4400\n",
            "Epoch 19/30\n",
            "----------\n",
            "Loss: 1.6808 Acc: 0.3800\n",
            "Epoch 20/30\n",
            "----------\n",
            "Loss: 1.4145 Acc: 0.5100\n",
            "Epoch 21/30\n",
            "----------\n",
            "Loss: 1.4748 Acc: 0.5200\n",
            "Epoch 22/30\n",
            "----------\n",
            "Loss: 1.4177 Acc: 0.5200\n",
            "Epoch 23/30\n",
            "----------\n",
            "Loss: 1.2628 Acc: 0.5500\n",
            "Epoch 24/30\n",
            "----------\n",
            "Loss: 1.2354 Acc: 0.6000\n",
            "Epoch 25/30\n",
            "----------\n",
            "Loss: 1.5453 Acc: 0.5000\n",
            "Epoch 26/30\n",
            "----------\n",
            "Loss: 1.3188 Acc: 0.5500\n",
            "Epoch 27/30\n",
            "----------\n",
            "Loss: 1.5507 Acc: 0.4600\n",
            "Epoch 28/30\n",
            "----------\n",
            "Loss: 1.2287 Acc: 0.5700\n",
            "Epoch 29/30\n",
            "----------\n",
            "Loss: 1.3715 Acc: 0.5400\n",
            "Epoch 30/30\n",
            "----------\n",
            "Loss: 1.3568 Acc: 0.5500\n",
            "Time for training is 8.85374903678894\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuDClhU0LE3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5d1daed-ab4f-4dbe-d0d0-c610aa52effe"
      },
      "source": [
        "pre_resnet.eval()\n",
        "start = time.time()\n",
        "with torch.no_grad():\n",
        "    corrects = 0.0\n",
        "    for images, labels in test_batch_dataloader:\n",
        "        images, labels = images.to(device),labels.to(device)\n",
        "        outputs = pre_resnet(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "print('Test accuracy is {}'.format(100 * corrects.double() / len(testset)))        \n",
        "print('Time for test is {}'.format(time.time()-start))"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy is 48.17\n",
            "Time for test is 22.597277641296387\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6GnLqzVLE3c"
      },
      "source": [
        "| Model | Number of  epochs  | Train accuracy | Test accuracy | Preprocess |\n",
        "|------|------|------|------|------|\n",
        "|   Densenet121  | 30 | 65.00% | 21.54% | different with pre-trained model |\n",
        "|   Densenet121  | 30 | 54.00% | 50.60% | same with pre-trained model(i.e. with augmentation) |\n",
        "|   Resnet18  | 30 | 55.00% | 48.17%| same with pre-trained model(i.e. with augmentation) |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8x7ppJF6mdb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhjUyVyK6mdb"
      },
      "source": [
        "# Incorporating *a priori*\n",
        "Geometrical *a priori* are appealing for image classification tasks. For now, we only consider linear transformations $\\mathcal{T}$ of the inputs $x:\\mathbb{S}^2\\rightarrow\\mathbb{R}$ where $\\mathbb{S}$ is the support of an image, meaning that:\n",
        "\n",
        "$$\\forall u\\in\\mathbb{S}^2,\\mathcal{T}(\\lambda x+\\mu y)(u)=\\lambda \\mathcal{T}(x)(u)+\\mu \\mathcal{T}(y)(u)\\,.$$\n",
        "\n",
        "For instance if an image had an infinite support, a translation $\\mathcal{T}_a$ by $a$ would lead to:\n",
        "\n",
        "$$\\forall u, \\mathcal{T}_a(x)(u)=x(u-a)\\,.$$\n",
        "\n",
        "Otherwise, one has to handle several boundary effects.\n",
        "\n",
        "__Question 5 (1.5 points):__ Explain the issues when dealing with translations, rotations, scaling effects, color changes on $32\\times32$ images. Propose several ideas to tackle them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0IQ0EsVW1tk"
      },
      "source": [
        "**The most common issue in dealing with these data augmentation methods is their safety of application, which refers to its likelihood of preserving the label post-transformation.**\r\n",
        "\r\n",
        "\r\n",
        "To be specific, \r\n",
        "1. Translations is very  useful in avoiding positional bias in the data, but in some tasks such as medical image analysis, the biases distancing the training data from the testing data are more complex than positional and translational variances.; \r\n",
        "2. Slight rotations could be useful on digit recognition tasks but a huge rotation could harm data auccuracy; \r\n",
        "3. Outward scaling effects might cause lose of image information; \r\n",
        "4. Color changes may discard important color information and thus are not always a label-preserving transformation. For example, when decreasing the pixel values of an image to simulate a darker environment, it may become impossible to see the objects in the image. \r\n",
        "\r\n",
        "Ideas to tackle these issues are that \r\n",
        "\r\n",
        "1. Use priori experience to choose reasonable data augmentation methods manually\r\n",
        "2. Use data augmentations methods based on Deep Learning such as AutoAugment and RandAugment\r\n",
        "\r\n",
        "\r\n",
        "Reference: A survey on Image Data Augmentation for Deep Learning (2019)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqQkQx9x6mdb"
      },
      "source": [
        "## Data augmentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "titONwCC6mdb"
      },
      "source": [
        "__Question 6 (3 points):__ Propose a set of geometric transformation beyond translation, and incorporate them in your training pipeline. Train the model of the __Question 3__ with them and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "pc-WVuqr6mdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c35b63c-6ecf-43eb-d178-5b38554f7187"
      },
      "source": [
        "import PIL\r\n",
        "transform_train_new = transforms.Compose([transforms.RandomCrop(32, padding=4),torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\r\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\r\n",
        "    torchvision.transforms.RandomRotation(20, resample=PIL.Image.BILINEAR),\r\n",
        "    transforms.ToTensor(),transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\r\n",
        "\r\n",
        "\r\n",
        "trainset_new = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train_new)\r\n",
        "first100 = torch.utils.data.Subset(trainset_new,list(range(0,100)))\r\n",
        "train_batch_dataloader_new = torch.utils.data.DataLoader(first100,batch_size=20,drop_last=False,shuffle=True)\r\n",
        "#testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\r\n",
        "val_batch_dataloader = torch.utils.data.DataLoader(testset,batch_size=20,drop_last=False,shuffle=False)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgOvqXbWt4y6"
      },
      "source": [
        "from torch.optim import lr_scheduler\r\n",
        "model_2 = resnet18().to(device)\r\n",
        "optimizer = torch.optim.SGD(model_2.parameters(), lr=0.01, weight_decay=5e-4) #momentum=0.9,\r\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "epochs = 30"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-lKOmmINu9mT",
        "outputId": "f7f58d5a-a935-4fb2-fe5b-cd97d6743534"
      },
      "source": [
        "import numpy as np\r\n",
        "save_folder = 'drive/My Drive/ADL/DA'\r\n",
        "total_train_losses = []\r\n",
        "total_val_losses = []\r\n",
        "\r\n",
        "for epoch in range(epochs): # loop over the dataset multiple times\r\n",
        "    model_2.train()\r\n",
        "    train_losses = []\r\n",
        "    confusion_matrix.reset()\r\n",
        "    for i, data in enumerate(train_batch_dataloader_new): \r\n",
        "        # get the inputs\r\n",
        "        inputs, labels = data\r\n",
        "        inputs = inputs.to(device)\r\n",
        "        labels = labels.to(device)\r\n",
        "        \r\n",
        "        # zero the parameter gradients\r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        # forward + backward + optimize\r\n",
        "        output = model_2(inputs)\r\n",
        "        loss = criterion(output, labels)\r\n",
        "        loss.backward()        \r\n",
        "        optimizer.step()\r\n",
        "        scheduler.step()\r\n",
        "\r\n",
        "        train_losses.append(loss.item())\r\n",
        "        confusion_matrix.add(output.data.squeeze(), labels.long())\r\n",
        "        # print statistics\r\n",
        "        if i % 5 == 0:\r\n",
        "            print('Train (epoch {}/{}) [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n",
        "                epoch, epochs, i, len(train_batch_dataloader_new),100.*i/len(train_batch_dataloader_new), loss.item()))\r\n",
        "\r\n",
        "    train_acc=(np.trace(confusion_matrix.conf)/float(np.ndarray.sum(confusion_matrix.conf))) *100\r\n",
        "    train_loss_mean = np.mean(train_losses)\r\n",
        "    total_train_losses.append(train_loss_mean)\r\n",
        "    confusion_matrix.reset()\r\n",
        "\r\n",
        "     ##VALIDATION##\r\n",
        "    with torch.no_grad():\r\n",
        "      model_2.eval()\r\n",
        "      val_losses = []\r\n",
        "\r\n",
        "      for i, data in enumerate(val_batch_dataloader):\r\n",
        "          inputs, labels = data\r\n",
        "          inputs = inputs.to(device)\r\n",
        "          labels = labels.to(device)\r\n",
        "          outputs=model_2(inputs)\r\n",
        "          loss=criterion(outputs, labels)\r\n",
        "          val_losses.append(loss.item())\r\n",
        "\r\n",
        "          confusion_matrix.add(outputs.data.squeeze(), labels)\r\n",
        "          val_losses.append(loss.item())\r\n",
        "\r\n",
        "    print('Confusion Matrix:')\r\n",
        "    print(confusion_matrix.conf)\r\n",
        "\r\n",
        "    val_acc=(np.trace(confusion_matrix.conf)/float(np.ndarray.sum(confusion_matrix.conf))) *100\r\n",
        "    val_loss_mean = np.mean(val_losses)\r\n",
        "    total_val_losses.append(val_loss_mean)\r\n",
        "\r\n",
        "    print('TRAIN_LOSS: ', '%.3f' % train_loss_mean, 'TRAIN_ACC: ', '%.3f' % train_acc)\r\n",
        "    print('VAL_LOSS: ', '%.3f' % val_loss_mean, 'VAL_ACC: ', '%.3f' % val_acc)\r\n",
        "    confusion_matrix.reset()\r\n",
        "\r\n",
        "    torch.save(model_2.state_dict(), save_folder + '/model_{}.pt'.format(epoch))\r\n",
        "\r\n",
        "save_graph(total_train_losses, total_val_losses, epochs)\r\n",
        "\r\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train (epoch 0/30) [0/5 (0%)]\tLoss: 2.416271\n",
            "Confusion Matrix:\n",
            "[[   0    0    0    0    0    0    0 1000    0    0]\n",
            " [   0    0    0    0    0    0    0 1000    0    0]\n",
            " [   0    0    0    0    0    0    0 1000    0    0]\n",
            " [   0    0    0    0    0    0    0 1000    0    0]\n",
            " [   0    0    0    0    0    0    0 1000    0    0]\n",
            " [   0    0    0    0    0    0    0 1000    0    0]\n",
            " [   0    0    0    0    0    0    0 1000    0    0]\n",
            " [   0    0    0    0    0    0    0 1000    0    0]\n",
            " [   0    0    0    0    0    0    0 1000    0    0]\n",
            " [   0    0    0    0    0    0    0 1000    0    0]]\n",
            "TRAIN_LOSS:  2.369 TRAIN_ACC:  6.000\n",
            "VAL_LOSS:  2.311 VAL_ACC:  10.000\n",
            "Train (epoch 1/30) [0/5 (0%)]\tLoss: 2.199887\n",
            "Confusion Matrix:\n",
            "[[   0    0    0    0    0    0    0 1000    0    0]\n",
            " [   0    0    0    0    0    0    0 1000    0    0]\n",
            " [   0    0    0    0    1    0    0  999    0    0]\n",
            " [   0    0    0    0    1    0    0  999    0    0]\n",
            " [   0    0    0    0    2    0    0  998    0    0]\n",
            " [   0    0    0    0    1    0    0  999    0    0]\n",
            " [   0    0    0    0    1    0    0  999    0    0]\n",
            " [   0    0    0    0    0    0    0 1000    0    0]\n",
            " [   0    0    0    0    0    0    0 1000    0    0]\n",
            " [   0    0    0    0    0    0    0 1000    0    0]]\n",
            "TRAIN_LOSS:  2.284 TRAIN_ACC:  12.000\n",
            "VAL_LOSS:  2.322 VAL_ACC:  10.020\n",
            "Train (epoch 2/30) [0/5 (0%)]\tLoss: 2.067516\n",
            "Confusion Matrix:\n",
            "[[  0   0   0   0 740   0   0 260   0   0]\n",
            " [  0   0   0   0 851   0   0 149   0   0]\n",
            " [  0   0   0   0 918   0   0  82   0   0]\n",
            " [  0   0   0   0 937   0   0  63   0   0]\n",
            " [  0   0   0   0 969   0   0  31   0   0]\n",
            " [  0   0   0   0 939   0   0  61   0   0]\n",
            " [  0   0   0   0 978   0   0  22   0   0]\n",
            " [  0   0   0   0 926   0   0  74   0   0]\n",
            " [  0   0   0   0 796   0   0 204   0   0]\n",
            " [  0   0   0   0 883   0   0 117   0   0]]\n",
            "TRAIN_LOSS:  2.140 TRAIN_ACC:  22.000\n",
            "VAL_LOSS:  2.341 VAL_ACC:  10.430\n",
            "Train (epoch 3/30) [0/5 (0%)]\tLoss: 2.029253\n",
            "Confusion Matrix:\n",
            "[[  0   0 357  13 537   0   0  37   0  56]\n",
            " [  0   0 157  12 739   0   0  68   0  24]\n",
            " [  0   0 128   5 840   0   0  20   0   7]\n",
            " [  0   0 116   3 853   0   0  24   0   4]\n",
            " [  0   0  43   0 943   0   0  14   0   0]\n",
            " [  0   0 132   1 846   0   0  19   0   2]\n",
            " [  0   0  30   2 957   0   0  11   0   0]\n",
            " [  0   0  74   2 895   0   0  25   0   4]\n",
            " [  0   0 259   6 648   0   0  43   0  44]\n",
            " [  0   0 130  12 800   0   0  42   0  16]]\n",
            "TRAIN_LOSS:  2.104 TRAIN_ACC:  23.000\n",
            "VAL_LOSS:  2.364 VAL_ACC:  11.150\n",
            "Train (epoch 4/30) [0/5 (0%)]\tLoss: 2.057906\n",
            "Confusion Matrix:\n",
            "[[  0   0 323  10 495   0   0  81   0  91]\n",
            " [  0   0 117  25 712   0   0  90   0  56]\n",
            " [  0   0 111   5 837   0   0  33   0  14]\n",
            " [  0   0 114   8 836   0   0  34   0   8]\n",
            " [  0   0  36   2 940   0   0  21   0   1]\n",
            " [  0   0 144   4 820   0   0  29   0   3]\n",
            " [  0   0  31   0 953   0   0  14   0   2]\n",
            " [  0   0  74  14 873   0   0  26   0  13]\n",
            " [  0   0 216  15 618   0   0  50   0 101]\n",
            " [  0   0 121  20 749   0   0  62   0  48]]\n",
            "TRAIN_LOSS:  1.882 TRAIN_ACC:  36.000\n",
            "VAL_LOSS:  2.374 VAL_ACC:  11.330\n",
            "Train (epoch 5/30) [0/5 (0%)]\tLoss: 1.723769\n",
            "Confusion Matrix:\n",
            "[[  0   2 313  16 371   0   0  64  35 199]\n",
            " [  0   9 220  40 494   0   0 116   0 121]\n",
            " [  0   0 140   8 751   0   0  42   7  52]\n",
            " [  0   1 176   7 739   0   0  55   0  22]\n",
            " [  0   0  74   5 887   0   0  28   0   6]\n",
            " [  0   1 225  16 697   0   0  46   0  15]\n",
            " [  0   0  67   2 898   0   0  22   0  11]\n",
            " [  0   2 104  23 771   0   0  49   0  51]\n",
            " [  0   0 249  27 458   0   0  58   5 203]\n",
            " [  0   3 160  43 525   0   0  98   0 171]]\n",
            "TRAIN_LOSS:  1.889 TRAIN_ACC:  35.000\n",
            "VAL_LOSS:  2.352 VAL_ACC:  12.680\n",
            "Train (epoch 6/30) [0/5 (0%)]\tLoss: 2.010024\n",
            "Confusion Matrix:\n",
            "[[  0  30 224  36 246   0   0  66  61 337]\n",
            " [  0 101 213  51 315   0   0 131   0 189]\n",
            " [  0   4 163  14 660   0   0  54  20  85]\n",
            " [  0  19 236  25 593   0   0  85   0  42]\n",
            " [  0   6  99   9 815   0   0  43   0  28]\n",
            " [  0  22 294  46 529   0   0  61   0  48]\n",
            " [  0   3  92   3 822   0   0  45   0  35]\n",
            " [  0  22 139  40 584   0   0  92   0 123]\n",
            " [  0  50 214  65 264   0   0  55  16 336]\n",
            " [  0  48 161  69 276   0   0 120   2 324]]\n",
            "TRAIN_LOSS:  1.883 TRAIN_ACC:  43.000\n",
            "VAL_LOSS:  2.307 VAL_ACC:  15.360\n",
            "Train (epoch 7/30) [0/5 (0%)]\tLoss: 1.855969\n",
            "Confusion Matrix:\n",
            "[[  1  94 132  46 170   0   0  40  96 421]\n",
            " [  0 265 149  68 185   0   0 102   1 230]\n",
            " [  1  28 150  34 585   0   0  60  31 111]\n",
            " [  0  64 216  64 478   0   0  93   2  83]\n",
            " [  0  19  95  18 751   0   0  55   4  58]\n",
            " [  0  86 256  82 428   0   0  64   1  83]\n",
            " [  0  16  95  15 764   0   0  58   0  52]\n",
            " [  0  81 131  62 446   0   0  80   1 199]\n",
            " [  0 162 116  76 152   0   0  25  32 437]\n",
            " [  0 177  99  68 159   0   0  61   3 433]]\n",
            "TRAIN_LOSS:  1.869 TRAIN_ACC:  42.000\n",
            "VAL_LOSS:  2.270 VAL_ACC:  17.760\n",
            "Train (epoch 8/30) [0/5 (0%)]\tLoss: 1.842159\n",
            "Confusion Matrix:\n",
            "[[  7 151  94  34 137   0   0  32  81 464]\n",
            " [  0 375 139  51 113   0   0  75   1 246]\n",
            " [  2  65 174  23 525   0   0  55  21 135]\n",
            " [  0 117 227  56 395   0   0 100   2 103]\n",
            " [  0  32 123  15 697   0   0  62   3  68]\n",
            " [  1 149 256  71 346   0   0  71   1 105]\n",
            " [  0  29 120  19 707   0   0  63   0  62]\n",
            " [  0 130 145  50 354   0   0  85   1 235]\n",
            " [  0 212 107  46 111   0   0  17  25 482]\n",
            " [  0 242  92  40 104   0   0  51   3 468]]\n",
            "TRAIN_LOSS:  1.802 TRAIN_ACC:  43.000\n",
            "VAL_LOSS:  2.270 VAL_ACC:  18.870\n",
            "Train (epoch 9/30) [0/5 (0%)]\tLoss: 1.698903\n",
            "Confusion Matrix:\n",
            "[[ 12 168  98  29 101   0   0  22  77 493]\n",
            " [  3 442 126  33  74   0   0  62   0 260]\n",
            " [  4  79 200  14 478   0   0  53  18 154]\n",
            " [  1 143 254  56 332   0   0  92   1 121]\n",
            " [  1  45 153  11 644   0   0  64   5  77]\n",
            " [  2 197 276  64 284   0   0  64   1 112]\n",
            " [  0  44 153  17 655   0   0  66   0  65]\n",
            " [  1 162 162  39 288   0   0  80   1 267]\n",
            " [  0 232 100  42  82   0   0  14  24 506]\n",
            " [  0 266  83  28  81   0   0  42   3 497]]\n",
            "TRAIN_LOSS:  1.785 TRAIN_ACC:  43.000\n",
            "VAL_LOSS:  2.281 VAL_ACC:  19.550\n",
            "Train (epoch 10/30) [0/5 (0%)]\tLoss: 1.850534\n",
            "Confusion Matrix:\n",
            "[[ 17 187  92  25  88   0   0  20  71 500]\n",
            " [  3 487 103  22  57   0   0  54   0 274]\n",
            " [  4  89 200  11 460   0   0  55  19 162]\n",
            " [  1 180 253  55 291   0   0  89   1 130]\n",
            " [  1  57 159   5 621   0   0  70   4  83]\n",
            " [  3 225 267  60 255   0   0  65   1 124]\n",
            " [  3  50 162  15 627   0   0  69   0  74]\n",
            " [  2 194 161  34 256   0   0  72   1 280]\n",
            " [  0 254  92  28  66   0   0  13  23 524]\n",
            " [  1 290  73  17  66   0   0  34   2 517]]\n",
            "TRAIN_LOSS:  1.824 TRAIN_ACC:  39.000\n",
            "VAL_LOSS:  2.293 VAL_ACC:  19.920\n",
            "Train (epoch 11/30) [0/5 (0%)]\tLoss: 1.905981\n",
            "Confusion Matrix:\n",
            "[[ 18 199  95  23  81   0   0  19  68 497]\n",
            " [  3 503 101  19  50   0   0  53   0 271]\n",
            " [  6  95 218  10 438   0   0  55  19 159]\n",
            " [  2 198 258  48 279   0   0  88   1 126]\n",
            " [  1  62 184   6 591   0   0  69   4  83]\n",
            " [  5 246 265  59 241   0   0  63   0 121]\n",
            " [  4  57 181  17 597   0   0  73   0  71]\n",
            " [  3 209 170  29 239   0   0  69   1 280]\n",
            " [  0 268  95  24  59   0   0  11  22 521]\n",
            " [  1 311  77  16  56   0   0  30   2 507]]\n",
            "TRAIN_LOSS:  1.765 TRAIN_ACC:  44.000\n",
            "VAL_LOSS:  2.296 VAL_ACC:  19.760\n",
            "Train (epoch 12/30) [0/5 (0%)]\tLoss: 1.682437\n",
            "Confusion Matrix:\n",
            "[[ 18 207  92  21  82   0   0  18  67 495]\n",
            " [  3 519  96  14  51   0   0  48   0 269]\n",
            " [  6 100 216   8 435   0   0  58  19 158]\n",
            " [  2 207 256  45 277   0   0  87   1 125]\n",
            " [  1  66 180   7 592   0   0  69   4  81]\n",
            " [  5 255 261  50 241   0   0  66   0 122]\n",
            " [  5  60 178  16 595   0   0  74   0  72]\n",
            " [  3 225 165  23 232   0   0  72   1 279]\n",
            " [  0 280  84  22  60   0   0  12  20 522]\n",
            " [  1 324  72  13  53   0   0  28   3 506]]\n",
            "TRAIN_LOSS:  1.757 TRAIN_ACC:  43.000\n",
            "VAL_LOSS:  2.295 VAL_ACC:  19.880\n",
            "Train (epoch 13/30) [0/5 (0%)]\tLoss: 1.889104\n",
            "Confusion Matrix:\n",
            "[[ 18 208  91  18  80   0   0  19  67 499]\n",
            " [  3 524  93   9  48   0   0  48   0 275]\n",
            " [ 10 100 216   8 431   0   0  59  19 157]\n",
            " [  2 205 259  45 271   0   0  89   1 128]\n",
            " [  1  67 184   5 583   0   0  72   4  84]\n",
            " [  6 266 259  43 233   0   0  67   0 126]\n",
            " [  5  65 175  16 590   0   0  77   0  72]\n",
            " [  3 226 168  20 227   0   0  73   1 282]\n",
            " [  0 284  81  19  57   0   0  11  22 526]\n",
            " [  1 325  71  11  49   0   0  26   2 515]]\n",
            "TRAIN_LOSS:  1.725 TRAIN_ACC:  46.000\n",
            "VAL_LOSS:  2.299 VAL_ACC:  19.960\n",
            "Train (epoch 14/30) [0/5 (0%)]\tLoss: 1.579366\n",
            "Confusion Matrix:\n",
            "[[ 18 200  96  19  77   0   0  19  70 501]\n",
            " [  3 509  95  19  46   0   0  52   0 276]\n",
            " [  6  97 226   7 425   0   0  61  19 159]\n",
            " [  2 199 275  50 258   0   0  88   1 127]\n",
            " [  1  65 198   8 570   0   0  71   4  83]\n",
            " [  6 249 279  53 219   0   0  66   0 128]\n",
            " [  5  59 186  20 579   0   0  78   0  73]\n",
            " [  3 217 177  23 218   0   0  77   1 284]\n",
            " [  0 267  90  22  56   0   0  13  24 528]\n",
            " [  1 310  78  14  47   0   0  30   2 518]]\n",
            "TRAIN_LOSS:  1.769 TRAIN_ACC:  45.000\n",
            "VAL_LOSS:  2.299 VAL_ACC:  19.920\n",
            "Train (epoch 15/30) [0/5 (0%)]\tLoss: 1.840469\n",
            "Confusion Matrix:\n",
            "[[ 17 194  98  21  77   0   0  18  70 505]\n",
            " [  3 507 104  14  45   0   0  49   0 278]\n",
            " [  6  96 238   6 417   0   0  59  18 160]\n",
            " [  1 197 281  46 255   0   0  87   1 132]\n",
            " [  1  63 204   6 567   0   0  70   4  85]\n",
            " [  4 247 289  47 218   0   0  66   0 129]\n",
            " [  4  58 193  19 574   0   0  78   0  74]\n",
            " [  3 214 190  18 214   0   0  75   1 285]\n",
            " [  0 270  92  19  54   0   0  12  23 530]\n",
            " [  1 307  83  13  44   0   0  28   2 522]]\n",
            "TRAIN_LOSS:  1.829 TRAIN_ACC:  39.000\n",
            "VAL_LOSS:  2.304 VAL_ACC:  19.950\n",
            "Train (epoch 16/30) [0/5 (0%)]\tLoss: 1.979883\n",
            "Confusion Matrix:\n",
            "[[ 18 204  97  17  76   0   0  19  68 501]\n",
            " [  3 513 103  11  44   0   0  51   0 275]\n",
            " [  8  96 239   6 415   0   0  59  18 159]\n",
            " [  2 198 288  43 251   0   0  88   1 129]\n",
            " [  1  64 209   5 562   0   0  71   4  84]\n",
            " [  6 250 293  46 215   0   0  66   0 124]\n",
            " [  5  60 197  14 572   0   0  79   0  73]\n",
            " [  3 220 189  18 212   0   0  74   1 283]\n",
            " [  0 275  94  18  52   0   0  12  22 527]\n",
            " [  1 314  84  11  43   0   0  29   2 516]]\n",
            "TRAIN_LOSS:  1.820 TRAIN_ACC:  42.000\n",
            "VAL_LOSS:  2.304 VAL_ACC:  19.870\n",
            "Train (epoch 17/30) [0/5 (0%)]\tLoss: 1.856086\n",
            "Confusion Matrix:\n",
            "[[ 19 205  97  18  75   0   0  17  65 504]\n",
            " [  3 523  96  10  46   0   0  46   0 276]\n",
            " [  9 101 228   6 418   0   0  58  18 162]\n",
            " [  2 212 275  44 249   0   0  86   1 131]\n",
            " [  1  67 200   8 565   0   0  69   4  86]\n",
            " [  6 268 274  44 216   0   0  62   0 130]\n",
            " [  5  69 186  17 574   0   0  75   0  74]\n",
            " [  3 230 182  19 213   0   0  68   1 284]\n",
            " [  0 284  86  16  54   0   0  11  20 529]\n",
            " [  1 325  75   9  44   0   0  25   2 519]]\n",
            "TRAIN_LOSS:  1.817 TRAIN_ACC:  36.000\n",
            "VAL_LOSS:  2.310 VAL_ACC:  19.860\n",
            "Train (epoch 18/30) [0/5 (0%)]\tLoss: 1.726214\n",
            "Confusion Matrix:\n",
            "[[ 18 213  91  17  76   0   0  19  65 501]\n",
            " [  3 531  90  10  46   0   0  47   0 273]\n",
            " [  6 104 223   7 421   0   0  58  18 163]\n",
            " [  2 217 267  44 253   0   0  89   0 128]\n",
            " [  1  68 192   8 571   0   0  71   4  85]\n",
            " [  6 271 262  42 221   0   0  69   0 129]\n",
            " [  5  69 180  15 580   0   0  78   0  73]\n",
            " [  3 233 173  18 215   0   0  73   1 284]\n",
            " [  0 287  84  17  54   0   0  10  20 528]\n",
            " [  1 333  75  11  43   0   0  26   2 509]]\n",
            "TRAIN_LOSS:  1.795 TRAIN_ACC:  42.000\n",
            "VAL_LOSS:  2.304 VAL_ACC:  19.890\n",
            "Train (epoch 19/30) [0/5 (0%)]\tLoss: 1.816738\n",
            "Confusion Matrix:\n",
            "[[ 19 219  90  16  74   0   0  18  64 500]\n",
            " [  3 538  84  11  44   0   0  47   0 273]\n",
            " [  7 106 218   9 418   0   0  61  18 163]\n",
            " [  2 223 259  51 244   0   0  92   0 129]\n",
            " [  1  74 189   9 562   0   0  76   3  86]\n",
            " [  6 279 255  46 215   0   0  70   0 129]\n",
            " [  5  74 179  15 571   0   0  84   0  72]\n",
            " [  3 239 165  22 211   0   0  77   1 282]\n",
            " [  0 288  82  18  54   0   0  11  19 528]\n",
            " [  1 337  72  11  41   0   0  26   2 510]]\n",
            "TRAIN_LOSS:  1.768 TRAIN_ACC:  43.000\n",
            "VAL_LOSS:  2.306 VAL_ACC:  19.940\n",
            "Train (epoch 20/30) [0/5 (0%)]\tLoss: 1.607438\n",
            "Confusion Matrix:\n",
            "[[ 18 211  88  18  73   0   0  21  68 503]\n",
            " [  3 531  84  11  46   0   0  48   0 277]\n",
            " [  6 107 218   9 418   0   0  60  18 164]\n",
            " [  2 218 255  53 248   0   0  93   1 130]\n",
            " [  1  70 187   9 566   0   0  76   4  87]\n",
            " [  6 269 249  49 220   0   0  74   0 133]\n",
            " [  5  64 179  20 575   0   0  84   0  73]\n",
            " [  3 231 161  25 213   0   0  80   1 286]\n",
            " [  0 283  76  19  54   0   0  13  22 533]\n",
            " [  1 327  69  14  42   0   0  30   2 515]]\n",
            "TRAIN_LOSS:  1.821 TRAIN_ACC:  44.000\n",
            "VAL_LOSS:  2.302 VAL_ACC:  20.030\n",
            "Train (epoch 21/30) [0/5 (0%)]\tLoss: 1.785375\n",
            "Confusion Matrix:\n",
            "[[ 20 205  94  19  74   0   0  18  66 504]\n",
            " [  4 527  90   9  47   0   0  47   0 276]\n",
            " [ 11 101 225   9 418   0   0  58  18 160]\n",
            " [  2 210 267  49 251   0   0  89   1 131]\n",
            " [  1  68 196   9 565   0   0  72   4  85]\n",
            " [  6 267 260  48 220   0   0  69   0 130]\n",
            " [  5  65 184  18 575   0   0  80   0  73]\n",
            " [  3 230 172  24 214   0   0  72   1 284]\n",
            " [  0 280  83  19  54   0   0  11  22 531]\n",
            " [  1 325  73  10  43   0   0  31   2 515]]\n",
            "TRAIN_LOSS:  1.793 TRAIN_ACC:  44.000\n",
            "VAL_LOSS:  2.306 VAL_ACC:  19.950\n",
            "Train (epoch 22/30) [0/5 (0%)]\tLoss: 1.836887\n",
            "Confusion Matrix:\n",
            "[[ 19 209  89  20  76   0   0  18  63 506]\n",
            " [  3 522  94  10  47   0   0  44   0 280]\n",
            " [ 12 102 217  10 427   0   0  55  17 160]\n",
            " [  2 210 257  50 264   0   0  85   0 132]\n",
            " [  1  67 189   7 575   0   0  69   4  88]\n",
            " [  6 267 261  52 223   0   0  61   0 130]\n",
            " [  5  64 177  22 588   0   0  71   0  73]\n",
            " [  3 229 166  26 220   0   0  68   1 287]\n",
            " [  0 278  81  19  56   0   0  11  20 535]\n",
            " [  1 323  74  12  44   0   0  22   2 522]]\n",
            "TRAIN_LOSS:  1.784 TRAIN_ACC:  41.000\n",
            "VAL_LOSS:  2.311 VAL_ACC:  19.930\n",
            "Train (epoch 23/30) [0/5 (0%)]\tLoss: 1.785325\n",
            "Confusion Matrix:\n",
            "[[ 19 217  88  20  75   0   0  15  64 502]\n",
            " [  4 532  88  11  47   0   0  42   0 276]\n",
            " [ 13 104 217  10 427   0   0  53  17 159]\n",
            " [  2 214 266  53 251   0   0  83   0 131]\n",
            " [  1  69 191   9 571   0   0  67   4  88]\n",
            " [  6 273 261  51 222   0   0  59   0 128]\n",
            " [  5  68 181  22 581   0   0  70   0  73]\n",
            " [  3 232 168  29 217   0   0  65   1 285]\n",
            " [  0 286  79  17  56   0   0  10  20 532]\n",
            " [  1 335  71  12  44   0   0  22   2 513]]\n",
            "TRAIN_LOSS:  1.793 TRAIN_ACC:  40.000\n",
            "VAL_LOSS:  2.308 VAL_ACC:  19.900\n",
            "Train (epoch 24/30) [0/5 (0%)]\tLoss: 1.661833\n",
            "Confusion Matrix:\n",
            "[[ 21 220  95  19  74   0   0  14  67 490]\n",
            " [  4 542  90  12  45   0   0  41   0 266]\n",
            " [ 13 106 223  11 420   0   0  52  19 156]\n",
            " [  2 218 269  48 250   0   0  86   1 126]\n",
            " [  1  71 199   7 565   0   0  71   4  82]\n",
            " [  6 278 267  50 217   0   0  62   0 120]\n",
            " [  6  70 185  18 574   0   0  76   0  71]\n",
            " [  3 238 178  21 211   0   0  68   1 280]\n",
            " [  0 289  83  19  54   0   0  10  23 522]\n",
            " [  1 341  73  13  41   0   0  25   3 503]]\n",
            "TRAIN_LOSS:  1.811 TRAIN_ACC:  39.000\n",
            "VAL_LOSS:  2.302 VAL_ACC:  19.930\n",
            "Train (epoch 25/30) [0/5 (0%)]\tLoss: 1.792084\n",
            "Confusion Matrix:\n",
            "[[ 18 207  94  19  75   0   0  17  69 501]\n",
            " [  3 535  89  13  46   0   0  44   0 270]\n",
            " [  7 104 221   7 424   0   0  56  19 162]\n",
            " [  2 212 264  51 256   0   0  87   1 127]\n",
            " [  1  68 196   7 571   0   0  69   4  84]\n",
            " [  6 266 267  51 220   0   0  63   0 127]\n",
            " [  5  63 180  19 582   0   0  78   0  73]\n",
            " [  3 229 171  26 218   0   0  68   1 284]\n",
            " [  0 281  83  18  54   0   0  11  25 528]\n",
            " [  1 334  73  13  43   0   0  26   2 508]]\n",
            "TRAIN_LOSS:  1.813 TRAIN_ACC:  45.000\n",
            "VAL_LOSS:  2.298 VAL_ACC:  19.970\n",
            "Train (epoch 26/30) [0/5 (0%)]\tLoss: 1.711473\n",
            "Confusion Matrix:\n",
            "[[ 18 208  94  17  72   0   0  16  71 504]\n",
            " [  3 534  90   8  45   0   0  42   0 278]\n",
            " [  9 107 219   7 422   0   0  55  18 163]\n",
            " [  2 212 277  46 252   0   0  83   1 127]\n",
            " [  1  69 196   7 567   0   0  69   4  87]\n",
            " [  6 266 273  47 215   0   0  61   0 132]\n",
            " [  5  65 186  18 580   0   0  73   0  73]\n",
            " [  3 230 173  24 215   0   0  68   1 286]\n",
            " [  0 280  82  14  54   0   0  11  26 533]\n",
            " [  1 330  73  11  44   0   0  21   2 518]]\n",
            "TRAIN_LOSS:  1.760 TRAIN_ACC:  42.000\n",
            "VAL_LOSS:  2.302 VAL_ACC:  19.960\n",
            "Train (epoch 27/30) [0/5 (0%)]\tLoss: 2.008879\n",
            "Confusion Matrix:\n",
            "[[ 17 204  88  16  73   0   0  19  71 512]\n",
            " [  3 523  89   8  46   0   0  50   0 281]\n",
            " [  7 100 220   7 424   0   0  59  19 164]\n",
            " [  1 207 268  45 255   0   0  91   1 132]\n",
            " [  1  65 195   6 567   0   0  73   4  89]\n",
            " [  6 256 264  45 218   0   0  74   0 137]\n",
            " [  4  60 182  18 581   0   0  80   0  75]\n",
            " [  3 219 169  18 214   0   0  81   1 295]\n",
            " [  0 279  80  14  53   0   0  12  26 536]\n",
            " [  1 316  73  11  44   0   0  25   2 528]]\n",
            "TRAIN_LOSS:  1.791 TRAIN_ACC:  39.000\n",
            "VAL_LOSS:  2.295 VAL_ACC:  20.070\n",
            "Train (epoch 28/30) [0/5 (0%)]\tLoss: 1.712680\n",
            "Confusion Matrix:\n",
            "[[ 17 203  94  19  73   0   0  20  73 501]\n",
            " [  3 521  93  11  46   0   0  53   0 273]\n",
            " [  6 103 222   6 421   0   0  61  20 161]\n",
            " [  1 207 267  46 255   0   0  95   1 128]\n",
            " [  1  66 195   6 568   0   0  76   4  84]\n",
            " [  4 251 264  48 223   0   0  79   0 131]\n",
            " [  4  61 183  17 579   0   0  82   0  74]\n",
            " [  3 220 176  21 215   0   0  80   1 284]\n",
            " [  0 278  85  19  53   0   0  13  24 528]\n",
            " [  1 325  76  13  42   0   0  31   3 509]]\n",
            "TRAIN_LOSS:  1.793 TRAIN_ACC:  39.000\n",
            "VAL_LOSS:  2.289 VAL_ACC:  19.870\n",
            "Train (epoch 29/30) [0/5 (0%)]\tLoss: 1.994921\n",
            "Confusion Matrix:\n",
            "[[ 17 196  93  19  75   0   0  20  70 510]\n",
            " [  3 514  93   9  46   0   0  52   0 283]\n",
            " [  7  99 225   5 423   0   0  59  18 164]\n",
            " [  1 205 267  48 255   0   0  90   0 134]\n",
            " [  1  64 196   6 567   0   0  74   4  88]\n",
            " [  5 249 268  49 222   0   0  72   0 135]\n",
            " [  5  61 181  16 583   0   0  80   0  74]\n",
            " [  3 215 172  20 217   0   0  81   1 291]\n",
            " [  0 275  81  15  54   0   0  13  25 537]\n",
            " [  1 313  75  13  43   0   0  27   2 526]]\n",
            "TRAIN_LOSS:  1.798 TRAIN_ACC:  42.000\n",
            "VAL_LOSS:  2.296 VAL_ACC:  20.030\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in76F01D9c-U"
      },
      "source": [
        "Resnet18 from scratch with data augmentatioon:\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "| Model | Number of  epochs  | Train accuracy | Test accuracy |\r\n",
        "|------|------|------|------|\r\n",
        "|   Resnet18(with data augmentation) | 30 | 42.00 | 20.03 |\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1NU79pP6mdc"
      },
      "source": [
        "# Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoix1OOS6mdc"
      },
      "source": [
        "__Question 7 (5 points):__ Write a short report explaining the pros and the cons of each methods that you implemented. 25% of the grade of this project will correspond to this question, thus, it should be done carefully. In particular, please add a plot that will summarize all your numerical results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_Rb-l0dT-wt"
      },
      "source": [
        "**1.Baseline:Resnet18**\r\n",
        "\r\n",
        "---\r\n",
        "Advantage of Resnet(18):\r\n",
        "\r\n",
        "\r\n",
        "1.   Reducing the effect of Vanishing Gradient Problem\r\n",
        "2.   Instead of widen the network, increasing depth of the network results in less extra parameters\r\n",
        "3.   Accelerate the speed of training of the deep networks\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Disadventage of Resnet(18):\r\n",
        "1.   Increased complexity of architecture\r\n",
        "2.   Adding skip level connections for which you have take into account the dimensionality between the different layers\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1AUCfFgUY-h"
      },
      "source": [
        "**2.Different pretrained model(Dense101 and Resnet18)**\r\n",
        "\r\n",
        "---\r\n",
        "Advantages: \r\n",
        "1.   improve the accuracy of the prediction even when using a small amount of training data \r\n",
        "2.   significantly reduce the training time since we only train a part of layers\r\n",
        "3.   There are various open source models. They are trained on large datasets and finetuned. And they show good generalization abilities.\r\n",
        "4.   The usage of the pre-trained model is easy and flexible, we can adjust it aiming at our objective and dataset.\r\n",
        "\r\n",
        "Disadventages: \r\n",
        "1.   There is a limitation on their applications. In other words, we need to carefully consider how similar our data and task are to those of the pre-trained model. For example, the pre-trained model is trained on the ImageNet dataset, it can learn the features of the specific 1000 classes. Thus, it can be used for the tasks with the similar dataset. However it can't be introduced in an unrelated dataset.\r\n",
        "2.   The architucture of the model can't be changed, only light tuning is possible.\r\n",
        "3.   There is a limitation on the step of the preprocess since when we use a pre-trained model, we need to keep the preprocessing method same. This idea is well verified by pur experience in this assignment.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRithluBVMao"
      },
      "source": [
        "**3.Resnet with data augmentation**\r\n",
        "\r\n",
        "---\r\n",
        "Advantage of Data augmentation for Neural Network:\r\n",
        "\r\n",
        "\r\n",
        "1.   Each epoch is trained on the original images that have been changed.In our case, training 30 epochs is equivalent to training 30*100=3000 different images\r\n",
        "2.   The addition of images allows the neural network to learn more generalized features\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Disadventage of Data augmentation for Neural Network:\r\n",
        "1.   A method of data enhancement must be found to fit the dataset.\r\n",
        "In our case, the data augmentation of resnet18 is not a big improvement compared to baseline\r\n",
        "2.   In some cases, the wrong method of data enhancement can lead to a reduction in accuracy\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2-jJE0KXajq"
      },
      "source": [
        "**4.Semi-supervised model**\r\n",
        "\r\n",
        "---\r\n",
        "Advantages:\r\n",
        "*  it could self learn the unlabeled large dataset using labeled dataset and \r\n",
        "then put them in the model again and self train to get higher accuracy. \r\n",
        "\r\n",
        "Disadvantages:\r\n",
        "*  The cons are obvious, since semi-supervised has strict hypothesis that labeled train dataset and unlabeled test dataset are supposed to have similar classes distribution, the training process is unstable and need longer time to train the model and have a ideal result. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyVzJwuKZ45Y"
      },
      "source": [
        "| Model | Number of  epochs  | Train accuracy | Test accuracy |\r\n",
        "|------|------|------|------|\r\n",
        "|   Resnet18 | 30(best test acc got on epoch=18) | 40.00 | 17.67% |\r\n",
        "|   Resnet18(pre-trained model[with the same augmentation method as dataset of pretrained model])  | 30 | 55.00% | 48.17%|   \r\n",
        "|   Resnet18(with data augmentation) | 30 | 42.00 | 20.03% |\r\n",
        "|   Mixmatch | 1000 | xxx | xxx |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_aeHApn6mdd"
      },
      "source": [
        "# Weak supervision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE1PnXbb6mdd"
      },
      "source": [
        "__Bonus \\[open\\] question (up to 3 points):__ Pick a weakly supervised method that will potentially use $\\mathcal{X}\\cup\\mathcal{X}_{\\text{train}}$ to train a representation (a subset of $\\mathcal{X}$ is also fine). Evaluate it and report the accuracies. You should be careful in the choice of your method, in order to avoid heavy computational effort."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1HFcw6EYj5v"
      },
      "source": [
        "The weakly supervised method that we take is MixMatch, defined in the paper MixMatch: A Holistic Approach to Semi-Supervised Learning(2019). \r\n",
        "\r\n",
        "MixMatch could be a powerful method in semi-supervised tasks as shown in the paper. MixMatch do do data augmentation to obtain K new data for unlabeled data and input these K new data into the same classifier to get different predicted classification probabilities. Then MixMatch uses an algorithm (Sharpen) to make the average variance of multiple probability distributions smaller, the prediction results more self-consistent, and the system entropy smaller. \r\n",
        "\r\n",
        "The code is mostly from the github resources: https://github.com/YU1ut/MixMatch-pytorch. We change the labeled and unlabeled train dataset in the python file *cifar10.py* (attached in the files *MixMatch-pytorch*) in order to meet the requirement of train data in this project.\r\n",
        "\r\n",
        "The evaluation results of the whole test dataset is list as follows.\r\n",
        "\r\n",
        "| Model | Number of  epochs  |  Test accuracy within Number of  epochs|\r\n",
        "|------ |--------------------|------|\r\n",
        "| MixMatch | 10 | 36.88 |\r\n",
        "| MixMatch | 100 | 45.68| \r\n",
        "| MixMatch | 400 | 55.49 |\r\n",
        "| MixMatch | 1000 |  | \r\n",
        "\r\n",
        "In our training process, the loss decrease and accuracy improvement is unstable (for more details, please check the *log.txt* in the files *MixMatch-pytorch*.). One possible reason is that the data distribution of the first 100 sampels are not dentical with the whole test dataset, which could be a basic hypothesis of semi-supervised algorithms. Besides, to have a ideal accuracy in test dataset as in the paper and github, we need to run the model for more than 1000 epochs and it's quite time-consuming. \r\n",
        "\r\n",
        "Therefore, we conclude the well-designed dataset with balanced distribution between train dataset and test dataset and a long training time is required if we want to have a higher accuracy, ideally 70-80% in test dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjmadNdK6mdd"
      },
      "source": [
        ""
      ]
    }
  ]
}